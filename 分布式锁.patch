Index: boot_redis02/src/main/java/com/atguigu/redis/util/RedisUtils.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- boot_redis02/src/main/java/com/atguigu/redis/util/RedisUtils.java	(revision daf4ff2ec2eebaa1aaf648cc1fcf5978f14fb9cc)
+++ boot_redis02/src/main/java/com/atguigu/redis/util/RedisUtils.java	(date 1622860171159)
@@ -16,7 +16,7 @@
         JedisPoolConfig jedisPoolConfig=new JedisPoolConfig();
         jedisPoolConfig.setMaxTotal(20);
         jedisPoolConfig.setMaxIdle(10);
-        jedisPool=new JedisPool(jedisPoolConfig,"192.168.111.147",6379);
+        jedisPool=new JedisPool(jedisPoolConfig,"192.168.0.122",6379);
     }
 
     public static Jedis getJedis() throws Exception {
@@ -25,52 +25,4 @@
         }
         throw new Exception("Jedispool is not ok");
     }
-
-
-    public static void main(String[] args) throws Exception
-    {
-        /*
-        Jedis jedis = null;
-        try
-        {
-            jedis = RedisUtils.getJedis();
-            System.out.println(jedis);
-
-            jedis.set("k1","abc");
-            String result = jedis.get("k1");
-            System.out.println("-----result: "+result);
-        }catch (Exception e){
-            e.printStackTrace();
-        }finally {
-            if (jedis != null && jedis.isConnected()) {
-                //不是关闭，而是返回给连接池
-                jedis.close();
-                jedis = null;
-                //System.out.println(jedis.quit());
-                //System.out.println(jedis.isConnected());
-                //jedis.disconnect();
-                //System.out.println(jedis.isConnected());
-                //jedis.shutdown();//不可以用
-            }
-        }*/
-
-        try(Jedis jedis = RedisUtils.getJedis())
-        {
-            System.out.println(jedis);
-
-            jedis.set("k1","xxx2");
-            String result = jedis.get("k1");
-            System.out.println("-----result: "+result);
-            System.out.println(RedisUtils.jedisPool.getNumActive());//1
-        }catch (Exception e){
-            e.printStackTrace();
-        }
-        System.out.println(RedisUtils.jedisPool.getNumActive());// 0
-
-
-        //-----close()方法执行完成后，目前我们的jedis链接是否还有 or  是个null了？
-
-
-
-    }
 }
Index: boot_redis02/src/main/java/com/atguigu/redis/controller/GoodController.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- boot_redis02/src/main/java/com/atguigu/redis/controller/GoodController.java	(revision daf4ff2ec2eebaa1aaf648cc1fcf5978f14fb9cc)
+++ boot_redis02/src/main/java/com/atguigu/redis/controller/GoodController.java	(date 1622861296795)
@@ -1,5 +1,6 @@
 package com.atguigu.redis.controller;
 
+import com.atguigu.redis.util.RedisUtils;
 import org.redisson.Redisson;
 import org.redisson.api.RLock;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -7,7 +8,9 @@
 import org.springframework.data.redis.core.StringRedisTemplate;
 import org.springframework.web.bind.annotation.GetMapping;
 import org.springframework.web.bind.annotation.RestController;
+import redis.clients.jedis.Jedis;
 
+import java.util.Collections;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.Lock;
@@ -199,7 +202,7 @@
     /**
      * @return java.lang.String
      * @Description: 设置key+过期时间成原子性操作
-     * 问题 张冠李戴 误删
+     * 问题 张冠李戴 误删 A加锁 执行业务逻辑-->过期时间到-->锁释放--->B线程进来--加锁-->A执行完业务逻辑-->删除锁，释放资源（但是这个时候其实是释放B的锁）
      * @author luoyong
      * @create 5:33 下午 2021/5/29
      * @last modify by [LuoYong 5:33 下午 2021/5/29 ]
@@ -231,8 +234,132 @@
     }
 
 
-    @GetMapping("/buy_goods")
-    public String buy_Goods() throws Exception {
+    /**
+     * @return java.lang.String
+     * @Description: 问题：判断和删除之前不是原子性操作-->使用lua脚本解决原子性问题
+     * @author luoyong
+     * @create 10:05 上午 2021/6/5
+     * @last modify by [LuoYong 10:05 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods007")
+    public String buyGoods007() throws Exception {
+        String value = UUID.randomUUID().toString() + Thread.currentThread().getName();
+        try {
+            //设置key+过期时间合成一条命令
+            Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(KEY, value, 10L, TimeUnit.SECONDS);
+            if (!flag) {
+                return "抢锁失败,please try again";
+            }
+
+            String result = stringRedisTemplate.opsForValue().get("goods:001");
+            int goodsNumber = result == null ? 0 : Integer.parseInt(result);
+            if (goodsNumber > 0) {
+                int realNumber = goodsNumber - 1;
+                stringRedisTemplate.opsForValue().set("goods:001", realNumber + "");
+                System.out.println("你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort);
+                return "你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort;
+            } else {
+                System.out.println("商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort);
+            }
+            return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
+        } finally {
+            if (stringRedisTemplate.opsForValue().get(KEY).equals(value)) {
+                stringRedisTemplate.delete(KEY);
+            }
+        }
+    }
+
+
+    /**
+     * @return java.lang.String
+     * @Description: 确保redisLock过期时间大于业务执行时间的问题?---->Redis分布式锁如何续期？
+     * @author luoyong
+     * @create 10:05 上午 2021/6/5
+     * @last modify by [LuoYong 10:05 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods008")
+    public String buyGoods008() throws Exception {
+        String value = UUID.randomUUID().toString() + Thread.currentThread().getName();
+        try {
+            //设置key+过期时间合成一条命令
+            Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(KEY, value, 10L, TimeUnit.SECONDS);
+            if (!flag) {
+                return "抢锁失败,please try again";
+            }
+
+            String result = stringRedisTemplate.opsForValue().get("goods:001");
+            int goodsNumber = result == null ? 0 : Integer.parseInt(result);
+            if (goodsNumber > 0) {
+                int realNumber = goodsNumber - 1;
+                stringRedisTemplate.opsForValue().set("goods:001", realNumber + "");
+                System.out.println("你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort);
+                return "你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort;
+            } else {
+                System.out.println("商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort);
+            }
+            return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
+        } finally {
+            Jedis jedis = RedisUtils.getJedis();
+            String script = "if redis.call('get', KEYS[1]) == ARGV[1] " +
+                    "then " +
+                    "return redis.call('del', KEYS[1]) " +
+                    "else " +
+                    "return 0 " +
+                    "end";
+            try {
+                Object result = jedis.eval(script, Collections.singletonList(KEY), Collections.singletonList(value));
+                if ("1".equals(result.toString())) {
+                    System.out.println("----- del redis_lock_key success");
+                } else {
+                    System.out.println("----- del redis_lock_key fail");
+                }
+            } finally {
+                if (null != jedis) {
+                    jedis.close();
+                }
+            }
+        }
+    }
+
+
+    /**
+     * @return java.lang.String
+     * @Description: 分布式锁 Redisson
+     * @author luoyong
+     * @create 10:44 上午 2021/6/5
+     * @last modify by [LuoYong 10:44 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods009")
+    public String buyGoods009() throws Exception {
+        RLock redissonLock = redisson.getLock(KEY);
+        redissonLock.lock();
+        try {
+
+            String result = stringRedisTemplate.opsForValue().get("goods:001");
+            int goodsNumber = result == null ? 0 : Integer.parseInt(result);
+            if (goodsNumber > 0) {
+                int realNumber = goodsNumber - 1;
+                stringRedisTemplate.opsForValue().set("goods:001", realNumber + "");
+                System.out.println("你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort);
+                return "你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort;
+            } else {
+                System.out.println("商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort);
+            }
+            return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
+        } finally {
+            redissonLock.unlock();
+        }
+    }
+
+    /**
+     * @return java.lang.String
+     * @Description: 分布式锁 Redisson
+     * @author luoyong
+     * @create 10:44 上午 2021/6/5
+     * @last modify by [LuoYong 10:44 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods010")
+    public String buyGoods010() throws Exception {
         RLock redissonLock = redisson.getLock(KEY);
         redissonLock.lock();
         try {
@@ -250,8 +377,10 @@
             return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
         } finally {
             if (redissonLock.isLocked() && redissonLock.isHeldByCurrentThread()) {
+                //防止太快了 删除了别人的锁
                 redissonLock.unlock();
             }
         }
     }
-}
\ No newline at end of file
+}
+
Index: redis/src/main/java/com/ly/redis/enums/DeleteFlagEnum.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/enums/DeleteFlagEnum.java	(date 1621178408897)
+++ redis/src/main/java/com/ly/redis/enums/DeleteFlagEnum.java	(date 1621178408897)
@@ -0,0 +1,107 @@
+package com.ly.redis.enums;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * 删除标记
+ * （1：已删除  0：未删除）
+ */
+public enum DeleteFlagEnum {
+    /**
+     * Y:已删除
+     */
+    DELETED("已删除", "Y"),
+    /**
+     * N：未删除
+     */
+    UNDELETED("未删除", "N");
+
+    /**
+     * 枚举值
+     */
+    private String value;
+    /**
+     * 描述
+     */
+    private String desc;
+
+    private DeleteFlagEnum(String desc, String value) {
+        this.value = value;
+        this.desc = desc;
+    }
+
+    public String getDesc() {
+        return desc;
+    }
+
+    public void setDesc(String desc) {
+        this.desc = desc;
+    }
+
+    public void setValue(String value) {
+        this.value = value;
+    }
+
+    public String getValue() {
+        return value;
+    }
+
+    public static DeleteFlagEnum getEnum(String value) {
+        DeleteFlagEnum resultEnum = null;
+        DeleteFlagEnum[] enumAry = DeleteFlagEnum.values();
+        for (int i = 0; i < enumAry.length; i++) {
+            if (enumAry[i].getValue().equals(value)) {
+                resultEnum = enumAry[i];
+                break;
+            }
+        }
+        return resultEnum;
+    }
+
+    public static Map<String, Map<String, Object>> toMap() {
+        DeleteFlagEnum[] ary = DeleteFlagEnum.values();
+        Map<String, Map<String, Object>> enumMap = new HashMap<String, Map<String, Object>>();
+        for (int num = 0; num < ary.length; num++) {
+            Map<String, Object> map = new HashMap<String, Object>();
+            String key = String.valueOf(getEnum(ary[num].getValue()));
+            map.put("value", String.valueOf(ary[num].getValue()));
+            map.put("desc", ary[num].getDesc());
+            enumMap.put(key, map);
+        }
+        return enumMap;
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static List toList() {
+        DeleteFlagEnum[] ary = DeleteFlagEnum.values();
+        List list = new ArrayList();
+        for (int i = 0; i < ary.length; i++) {
+            Map<String, String> map = new HashMap<String, String>();
+            map.put("value", String.valueOf(ary[i].getValue()));
+            map.put("desc", ary[i].getDesc());
+            list.add(map);
+        }
+        return list;
+    }
+
+    /**
+     * 取枚举的json字符串
+     *
+     * @return
+     */
+    public static String getJsonStr() {
+        DeleteFlagEnum[] enums = DeleteFlagEnum.values();
+        StringBuffer jsonStr = new StringBuffer("[");
+        for (DeleteFlagEnum senum : enums) {
+            if (!"[".equals(jsonStr.toString())) {
+                jsonStr.append(",");
+            }
+            jsonStr.append("{id:'").append(senum).append("',desc:'").append(senum.getDesc()).append("',value:'").append(senum.getValue()).append("'}");
+        }
+        jsonStr.append("]");
+        return jsonStr.toString();
+    }
+}
Index: redis/src/main/java/com/ly/redis/utils/BeanUtils.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/utils/BeanUtils.java	(date 1621177864064)
+++ redis/src/main/java/com/ly/redis/utils/BeanUtils.java	(date 1621177864064)
@@ -0,0 +1,156 @@
+package com.ly.redis.utils;
+
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.beans.BeansException;
+import org.springframework.beans.FatalBeanException;
+import org.springframework.util.Assert;
+
+import java.beans.PropertyDescriptor;
+import java.lang.reflect.Method;
+import java.lang.reflect.Modifier;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+
+/**
+ * obj和map的转换工具
+ * 
+ * @author Yang JiZhou
+ * @created 2017年5月25日 下午2:04:36
+ */
+@Slf4j
+public class BeanUtils extends org.springframework.beans.BeanUtils{
+
+    /**
+     * 复制属性
+     *
+     * @param source
+     * @param target
+     * @throws BeansException
+     */
+    public static void copyProperties(Object source, Object target) throws BeansException {
+        BeanUtils.copyProperties(source, target, null, (String[]) null);
+    }
+
+    /**
+     * 复制属性
+     *
+     * @param source
+     * @param target
+     * @param editable
+     * @throws BeansException
+     */
+    public static void copyProperties(Object source, Object target, Class<?> editable) throws BeansException {
+        copyProperties(source, target, editable, (String[]) null);
+    }
+
+    /**
+     * 复制属性
+     *
+     * @param source
+     * @param target
+     * @param ignoreProperties
+     * @throws BeansException
+     */
+    public static void copyProperties(Object source, Object target, String... ignoreProperties) throws BeansException {
+        copyProperties(source, target, null, ignoreProperties);
+    }
+
+    /**
+     * 复制属性
+     *
+     * @param source
+     * @param target
+     * @param editable
+     * @param ignoreProperties
+     * @throws BeansException
+     */
+    private static void copyProperties(Object source, Object target, Class<?> editable, String... ignoreProperties)
+            throws BeansException {
+
+        Assert.notNull(source, "Source must not be null");
+        Assert.notNull(target, "Target must not be null");
+
+        Class<?> actualEditable = target.getClass();
+        if (editable != null) {
+            if (!editable.isInstance(target)) {
+                throw new IllegalArgumentException("Target class [" + target.getClass().getName() +
+                        "] not assignable to Editable class [" + editable.getName() + "]");
+            }
+            actualEditable = editable;
+        }
+        PropertyDescriptor[] targetPds = getPropertyDescriptors(actualEditable);
+        List<String> ignoreList = (ignoreProperties != null ? Arrays.asList(ignoreProperties) : null);
+
+        for (PropertyDescriptor targetPd : targetPds) {
+            Method writeMethod = targetPd.getWriteMethod();
+            if (writeMethod != null && (ignoreList == null || !ignoreList.contains(targetPd.getName()))) {
+                PropertyDescriptor sourcePd = getPropertyDescriptor(source.getClass(), targetPd.getName());
+                if (sourcePd != null) {
+                    Method readMethod = sourcePd.getReadMethod();
+                    if (readMethod != null &&
+                            org.springframework.util.ClassUtils.isAssignable(writeMethod.getParameterTypes()[0], readMethod.getReturnType())) {
+                        try {
+                            if (!Modifier.isPublic(readMethod.getDeclaringClass().getModifiers())) {
+                                readMethod.setAccessible(true);
+                            }
+                            Object value = readMethod.invoke(source);
+                            // 这里判断以下value是否为空 当然这里也能进行一些特殊要求的处理 例如绑定时格式转换等等
+                            if (value != null) {
+                                if (!Modifier.isPublic(writeMethod.getDeclaringClass().getModifiers())) {
+                                    writeMethod.setAccessible(true);
+                                }
+                                writeMethod.invoke(target, value);
+                            }
+                        } catch (Throwable ex) {
+                            throw new FatalBeanException(
+                                    "Could not copy property '" + targetPd.getName() + "' from source to target", ex);
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    /**
+     * Clone 对象
+     *
+     * @param o
+     * @param clazz
+     * @param <T>
+     * @return
+     * @throws BeansException
+     */
+    public static <T> T clone(Object o, Class<T> clazz) {
+        try {
+            T t = clazz.newInstance();
+            if (null == o)
+                return null;
+            copyProperties(o, t);
+            return t;
+        } catch (Exception e) {
+            log.error("将目标转换为DTO对象，发生异常:", e);
+        }
+        return null;
+    }
+
+    /**
+     * Close出一个List
+     *
+     * @param os
+     * @param clazz
+     * @param <T>
+     * @return
+     * @throws BeansException
+     */
+    public static <T> List<T> cloneList(List<?> os, Class<T> clazz) {
+        List<T> ws = new ArrayList<>();
+        if (null == os)
+            return ws;
+        for (Object o : os) {
+            ws.add(clone(o, clazz));
+        }
+        return ws;
+    }
+}
Index: redis/src/main/java/com/ly/redis/dto/UserDTO.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/dto/UserDTO.java	(date 1621181097628)
+++ redis/src/main/java/com/ly/redis/dto/UserDTO.java	(date 1621181097628)
@@ -0,0 +1,172 @@
+package com.ly.redis.dto;
+
+import io.swagger.annotations.ApiModelProperty;
+
+import java.io.Serializable;
+import java.util.Date;
+
+/**
+ * Created by chenshiyang on 2017.
+ */
+public class UserDTO implements Serializable {
+
+
+    private Long id;
+
+    private String name;
+
+    private String accountName;
+
+    private String email;
+
+    private String phoneNumber;
+
+    private String password;
+
+    private String status;
+
+    private String updatePassword;
+
+    private String directSupervisor;
+
+    private String position;
+
+    private String wechatPlatformId;
+
+    private Long tenantId;
+
+    private Date gmtCreated;
+
+
+    @ApiModelProperty("主键id")
+    public Long getId() {
+        return this.id;
+    }
+
+    public UserDTO setId(Long value) {
+        this.id = value;
+        return this;
+    }
+
+    @ApiModelProperty("用户名称")
+    public String getName() {
+        return this.name;
+    }
+
+    public UserDTO setName(String value) {
+        this.name = value;
+        return this;
+    }
+
+    @ApiModelProperty("账户名")
+    public String getAccountName() {
+        return this.accountName;
+    }
+
+    public UserDTO setAccountName(String value) {
+        this.accountName = value;
+        return this;
+    }
+
+    @ApiModelProperty("邮箱")
+    public String getEmail() {
+        return this.email;
+    }
+
+    public UserDTO setEmail(String value) {
+        this.email = value;
+        return this;
+    }
+
+    @ApiModelProperty("手机")
+    public String getPhoneNumber() {
+        return this.phoneNumber;
+    }
+
+    public UserDTO setPhoneNumber(String value) {
+        this.phoneNumber = value;
+        return this;
+    }
+
+    @ApiModelProperty("密码")
+    public String getPassword() {
+        return this.password;
+    }
+
+    public UserDTO setPassword(String value) {
+        this.password = value;
+        return this;
+    }
+
+    @ApiModelProperty("状态")
+    public String getStatus() {
+        return this.status;
+    }
+
+    public UserDTO setStatus(String value) {
+        this.status = value;
+        return this;
+    }
+
+    @ApiModelProperty("是否修改过密码")
+    public String getUpdatePassword() {
+        return this.updatePassword;
+    }
+
+    public UserDTO setUpdatePassword(String value) {
+        this.updatePassword = value;
+        return this;
+    }
+
+    @ApiModelProperty("直属上级")
+    public String getDirectSupervisor() {
+        return this.directSupervisor;
+    }
+
+    public UserDTO setDirectSupervisor(String value) {
+        this.directSupervisor = value;
+        return this;
+    }
+
+    @ApiModelProperty("岗位")
+    public String getPosition() {
+        return this.position;
+    }
+
+    public UserDTO setPosition(String value) {
+        this.position = value;
+        return this;
+    }
+
+    @ApiModelProperty("微信平台id")
+    public String getWechatPlatformId() {
+        return this.wechatPlatformId;
+    }
+
+    public UserDTO setWechatPlatformId(String value) {
+        this.wechatPlatformId = value;
+        return this;
+    }
+
+    @ApiModelProperty("租户id")
+    public Long getTenantId() {
+        return this.tenantId;
+    }
+
+    public UserDTO setTenantId(Long value) {
+        this.tenantId = value;
+        return this;
+    }
+
+    @ApiModelProperty("创建时间")
+    public Date getGmtCreated() {
+        return this.gmtCreated;
+    }
+
+    public UserDTO setGmtCreated(Date value) {
+        this.gmtCreated = value;
+        return this;
+    }
+
+
+}
Index: redis/src/main/java/com/ly/redis/entity/Product.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/entity/Product.java	(date 1622261087155)
+++ redis/src/main/java/com/ly/redis/entity/Product.java	(date 1622261087155)
@@ -0,0 +1,28 @@
+package com.ly.redis.entity;
+
+import io.swagger.annotations.ApiModelProperty;
+import lombok.AllArgsConstructor;
+import lombok.Data;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-29 11:59 上午
+ *  * @last modify by [luoyong 2021-05-29 11:59 上午]
+ * @Description: 产品信息
+ **/
+@Data
+@AllArgsConstructor
+public class Product {
+
+    @ApiModelProperty("产品id")
+    private Long id;
+
+    @ApiModelProperty("产品名称")
+    private String name;
+
+    @ApiModelProperty("产品价格")
+    private Integer price;
+
+    @ApiModelProperty("产品详情")
+    private String detail;
+}
Index: redis/src/main/java/com/ly/redis/entity/UserDO.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/entity/UserDO.java	(date 1621178408892)
+++ redis/src/main/java/com/ly/redis/entity/UserDO.java	(date 1621178408892)
@@ -0,0 +1,225 @@
+package com.ly.redis.entity;
+
+
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.Table;
+import java.io.Serializable;
+import java.util.Date;
+
+/**
+ * 用户表
+ */
+@Table(name = "t_user")
+public class UserDO implements Serializable {
+
+    /**
+     * 主键id
+     */
+    @Id
+    @GeneratedValue(strategy = GenerationType.IDENTITY)
+    private Long id;
+
+    /**
+     * 用户名称
+     */
+    private String name;
+
+    /**
+     * 账户名
+     */
+    private String accountName;
+
+    /**
+     * 邮箱
+     */
+    private String email;
+
+    /**
+     * 手机
+     */
+    private String phoneNumber;
+
+    /**
+     * 密码
+     */
+    private String password;
+
+    /**
+     * 状态
+     */
+    private String status;
+
+    /**
+     * 是否修改过密码
+     */
+    private String updatePassword;
+
+    /**
+     * 直属上级
+     */
+    private String directSupervisor;
+
+    /**
+     * 岗位
+     */
+    private String position;
+
+    /**
+     * 微信平台id
+     */
+    private String wechatPlatformId;
+
+    /**
+     * 租户id
+     */
+    private Long tenantId;
+
+    /**
+     * 创建时间
+     */
+    private Date gmtCreated;
+
+    private String isDeleted;
+
+    private String creator;
+    private String modifier;
+    private Date gmtModified;
+
+    public Long getId() {
+        return id;
+    }
+
+    public void setId(Long id) {
+        this.id = id;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public void setName(String name) {
+        this.name = name;
+    }
+
+    public String getAccountName() {
+        return accountName;
+    }
+
+    public void setAccountName(String accountName) {
+        this.accountName = accountName;
+    }
+
+    public String getEmail() {
+        return email;
+    }
+
+    public void setEmail(String email) {
+        this.email = email;
+    }
+
+    public String getPhoneNumber() {
+        return phoneNumber;
+    }
+
+    public void setPhoneNumber(String phoneNumber) {
+        this.phoneNumber = phoneNumber;
+    }
+
+    public String getPassword() {
+        return password;
+    }
+
+    public void setPassword(String password) {
+        this.password = password;
+    }
+
+    public String getStatus() {
+        return status;
+    }
+
+    public void setStatus(String status) {
+        this.status = status;
+    }
+
+    public String getUpdatePassword() {
+        return updatePassword;
+    }
+
+    public void setUpdatePassword(String updatePassword) {
+        this.updatePassword = updatePassword;
+    }
+
+    public String getDirectSupervisor() {
+        return directSupervisor;
+    }
+
+    public void setDirectSupervisor(String directSupervisor) {
+        this.directSupervisor = directSupervisor;
+    }
+
+    public String getPosition() {
+        return position;
+    }
+
+    public void setPosition(String position) {
+        this.position = position;
+    }
+
+    public String getWechatPlatformId() {
+        return wechatPlatformId;
+    }
+
+    public void setWechatPlatformId(String wechatPlatformId) {
+        this.wechatPlatformId = wechatPlatformId;
+    }
+
+    public Long getTenantId() {
+        return tenantId;
+    }
+
+    public void setTenantId(Long tenantId) {
+        this.tenantId = tenantId;
+    }
+
+    public Date getGmtCreated() {
+        return gmtCreated;
+    }
+
+    public void setGmtCreated(Date gmtCreated) {
+        this.gmtCreated = gmtCreated;
+    }
+
+    public String getIsDeleted() {
+        return isDeleted;
+    }
+
+    public void setIsDeleted(String isDeleted) {
+        this.isDeleted = isDeleted;
+    }
+
+    public String getCreator() {
+        return creator;
+    }
+
+    public void setCreator(String creator) {
+        this.creator = creator;
+    }
+
+    public String getModifier() {
+        return modifier;
+    }
+
+    public void setModifier(String modifier) {
+        this.modifier = modifier;
+    }
+
+    public Date getGmtModified() {
+        return gmtModified;
+    }
+
+    public void setGmtModified(Date gmtModified) {
+        this.gmtModified = gmtModified;
+    }
+}
\ No newline at end of file
Index: redis/src/main/java/com/ly/redis/config/RedisConfig.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/config/RedisConfig.java	(date 1621176377062)
+++ redis/src/main/java/com/ly/redis/config/RedisConfig.java	(date 1621176377062)
@@ -0,0 +1,51 @@
+package com.ly.redis.config;
+
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.context.annotation.Bean;
+import org.springframework.context.annotation.Configuration;
+import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;
+import org.springframework.data.redis.serializer.StringRedisSerializer;
+
+import java.io.Serializable;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-16 10:44 下午
+ *  * @last modify by [luoyong 2021-05-16 10:44 下午]
+ * @Description: redis配置类
+ **/
+@Configuration
+@Slf4j
+public class RedisConfig {
+
+    /**
+     * @param lettuceConnectionFactory
+     * @return
+     * @Description: redis序列化的工具配置类，下面这个请一定开启配置
+     * 127.0.0.1:6379> keys *
+     * 1) "ord:102"  序列化过
+     * 2) "\xac\xed\x00\x05t\x00\aord:102"   野生，没有序列化过
+     * @author luoyong
+     * @create 10:45 下午 2021/5/16
+     * @last modify by [LuoYong 10:45 下午 2021/5/16 ]
+     */
+    @Bean
+    public RedisTemplate<String, Serializable> redisTemplate(LettuceConnectionFactory lettuceConnectionFactory) {
+        RedisTemplate<String, Serializable> redisTemplate = new RedisTemplate<>();
+
+        redisTemplate.setConnectionFactory(lettuceConnectionFactory);
+        //设置key序列化方式string
+        redisTemplate.setKeySerializer(new StringRedisSerializer());
+        //设置value的序列化方式json
+        redisTemplate.setValueSerializer(new GenericJackson2JsonRedisSerializer());
+
+        redisTemplate.setHashKeySerializer(new StringRedisSerializer());
+        redisTemplate.setHashValueSerializer(new GenericJackson2JsonRedisSerializer());
+
+        redisTemplate.afterPropertiesSet();
+
+        return redisTemplate;
+    }
+}
Index: redis/src/main/java/com/ly/redis/config/SwaggerConfig.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/config/SwaggerConfig.java	(date 1621176458541)
+++ redis/src/main/java/com/ly/redis/config/SwaggerConfig.java	(date 1621176458541)
@@ -0,0 +1,48 @@
+package com.ly.redis.config;
+
+import org.springframework.beans.factory.annotation.Value;
+import org.springframework.context.annotation.Bean;
+import org.springframework.context.annotation.Configuration;
+import springfox.documentation.builders.ApiInfoBuilder;
+import springfox.documentation.builders.PathSelectors;
+import springfox.documentation.builders.RequestHandlerSelectors;
+import springfox.documentation.service.ApiInfo;
+import springfox.documentation.spi.DocumentationType;
+import springfox.documentation.spring.web.plugins.Docket;
+import springfox.documentation.swagger2.annotations.EnableSwagger2;
+
+import java.text.SimpleDateFormat;
+import java.util.Date;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-16 10:46 下午
+ *  * @last modify by [luoyong 2021-05-16 10:46 下午]
+ * @Description: SwaggerConfig
+ **/
+@Configuration
+@EnableSwagger2
+public class SwaggerConfig {
+    @Value("${spring.swagger2.enabled}")
+    private Boolean enabled;
+
+    @Bean
+    public Docket createRestApi() {
+        return new Docket(DocumentationType.SWAGGER_2)
+                .apiInfo(apiInfo())
+                .enable(enabled)
+                .select()
+                .apis(RequestHandlerSelectors.basePackage("com.ly.redis"))
+                .paths(PathSelectors.any())
+                .build();
+    }
+
+    public ApiInfo apiInfo() {
+        return new ApiInfoBuilder()
+                .title("redis" + "\t" + new SimpleDateFormat("yyyy-MM-dd").format(new Date()))
+                .description("redis6")
+                .version("1.0")
+                .termsOfServiceUrl("xxxxxxxxxx")
+                .build();
+    }
+}
Index: redis/pom.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/pom.xml	(date 1622856285944)
+++ redis/pom.xml	(date 1622856285944)
@@ -0,0 +1,326 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.springframework.boot</groupId>
+        <artifactId>spring-boot-starter-parent</artifactId>
+        <version>2.3.10.RELEASE</version>
+        <relativePath/> <!-- lookup parent from repository -->
+    </parent>
+    <groupId>com.ly</groupId>
+    <artifactId>redis</artifactId>
+    <version>0.0.1-SNAPSHOT</version>
+    <name>redis</name>
+
+    <modules>
+        <module>boot_redis01</module>
+        <module>boot_redis02</module>
+    </modules>
+
+    <properties>
+        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+        <maven.compiler.source>1.8</maven.compiler.source>
+        <maven.compiler.target>1.8</maven.compiler.target>
+        <junit.version>4.12</junit.version>
+        <log4j.version>1.2.17</log4j.version>
+        <lombok.version>1.16.18</lombok.version>
+        <mysql.version>5.1.47</mysql.version>
+        <druid.version>1.1.16</druid.version>
+        <mapper.version>4.1.5</mapper.version>
+        <mybatis.spring.boot.version>1.3.0</mybatis.spring.boot.version>
+    </properties>
+
+    <dependencies>
+        <!--guava Google 开源的 Guava 中自带的布隆过滤器-->
+        <dependency>
+            <groupId>com.google.guava</groupId>
+            <artifactId>guava</artifactId>
+            <version>23.0</version>
+        </dependency>
+        <!-- redisson -->
+        <dependency>
+            <groupId>org.redisson</groupId>
+            <artifactId>redisson</artifactId>
+            <version>3.13.4</version>
+        </dependency>
+        <!--SpringBoot通用依赖模块-->
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-starter-web</artifactId>
+        </dependency>
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-starter-actuator</artifactId>
+        </dependency>
+        <!--swagger2-->
+        <dependency>
+            <groupId>io.springfox</groupId>
+            <artifactId>springfox-swagger2</artifactId>
+            <version>2.9.2</version>
+        </dependency>
+        <dependency>
+            <groupId>io.springfox</groupId>
+            <artifactId>springfox-swagger-ui</artifactId>
+            <version>2.9.2</version>
+        </dependency>
+        <!--SpringBoot与Redis整合依赖-->
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-starter-data-redis</artifactId>
+        </dependency>
+        <!--springCache-->
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-starter-cache</artifactId>
+        </dependency>
+        <!--springCache连接池依赖包-->
+        <dependency>
+            <groupId>org.apache.commons</groupId>
+            <artifactId>commons-pool2</artifactId>
+        </dependency>
+        <!-- jedis -->
+        <dependency>
+            <groupId>redis.clients</groupId>
+            <artifactId>jedis</artifactId>
+            <version>3.1.0</version>
+        </dependency>
+        <!--Mysql数据库驱动-->
+        <dependency>
+            <groupId>mysql</groupId>
+            <artifactId>mysql-connector-java</artifactId>
+            <version>5.1.47</version>
+        </dependency>
+        <!--SpringBoot集成druid连接池-->
+        <dependency>
+            <groupId>com.alibaba</groupId>
+            <artifactId>druid-spring-boot-starter</artifactId>
+            <version>1.1.10</version>
+        </dependency>
+        <dependency>
+            <groupId>com.alibaba</groupId>
+            <artifactId>druid</artifactId>
+            <version>${druid.version}</version>
+        </dependency>
+        <!--mybatis和springboot整合-->
+        <dependency>
+            <groupId>org.mybatis.spring.boot</groupId>
+            <artifactId>mybatis-spring-boot-starter</artifactId>
+            <version>${mybatis.spring.boot.version}</version>
+        </dependency>
+        <!-- 添加springboot对amqp的支持 -->
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-starter-amqp</artifactId>
+        </dependency>
+        <dependency>
+            <groupId>commons-codec</groupId>
+            <artifactId>commons-codec</artifactId>
+            <version>1.10</version>
+        </dependency>
+        <!--通用基础配置junit/devtools/test/log4j/lombok/hutool-->
+        <!--hutool-->
+        <dependency>
+            <groupId>cn.hutool</groupId>
+            <artifactId>hutool-all</artifactId>
+            <version>5.2.3</version>
+        </dependency>
+        <dependency>
+            <groupId>junit</groupId>
+            <artifactId>junit</artifactId>
+            <version>${junit.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-devtools</artifactId>
+            <scope>runtime</scope>
+            <optional>true</optional>
+        </dependency>
+        <dependency>
+            <groupId>org.springframework.boot</groupId>
+            <artifactId>spring-boot-starter-test</artifactId>
+            <scope>test</scope>
+        </dependency>
+        <dependency>
+            <groupId>log4j</groupId>
+            <artifactId>log4j</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>org.projectlombok</groupId>
+            <artifactId>lombok</artifactId>
+            <version>${lombok.version}</version>
+            <optional>true</optional>
+        </dependency>
+        <!--persistence-->
+        <dependency>
+            <groupId>javax.persistence</groupId>
+            <artifactId>persistence-api</artifactId>
+            <version>1.0.2</version>
+        </dependency>
+        <!--通用Mapper-->
+        <dependency>
+            <groupId>tk.mybatis</groupId>
+            <artifactId>mapper</artifactId>
+            <version>${mapper.version}</version>
+        </dependency>
+    </dependencies>
+
+    <!--    <dependencies>-->
+    <!--        &lt;!&ndash; jrebloom布隆过滤器 &ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>com.redislabs</groupId>-->
+    <!--            <artifactId>jrebloom</artifactId>-->
+    <!--            <version>2.1.0</version>-->
+    <!--            <exclusions>-->
+    <!--                <exclusion>-->
+    <!--                    <artifactId>jedis</artifactId>-->
+    <!--                    <groupId>redis.clients</groupId>-->
+    <!--                </exclusion>-->
+    <!--            </exclusions>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;SpringBoot通用依赖模块&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.springframework.boot</groupId>-->
+    <!--            <artifactId>spring-boot-starter-web</artifactId>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.springframework.boot</groupId>-->
+    <!--            <artifactId>spring-boot-starter-actuator</artifactId>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;swagger2&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>io.springfox</groupId>-->
+    <!--            <artifactId>springfox-swagger2</artifactId>-->
+    <!--            <version>2.9.2</version>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>io.springfox</groupId>-->
+    <!--            <artifactId>springfox-swagger-ui</artifactId>-->
+    <!--            <version>2.9.2</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;SpringBoot与Redis整合依赖&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.springframework.boot</groupId>-->
+    <!--            <artifactId>spring-boot-starter-data-redis</artifactId>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;springCache&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.springframework.boot</groupId>-->
+    <!--            <artifactId>spring-boot-starter-cache</artifactId>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;springCache连接池依赖包&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.apache.commons</groupId>-->
+    <!--            <artifactId>commons-pool2</artifactId>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash; jedis &ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>redis.clients</groupId>-->
+    <!--            <artifactId>jedis</artifactId>-->
+    <!--            <version>3.1.0</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;Mysql数据库驱动&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>mysql</groupId>-->
+    <!--            <artifactId>mysql-connector-java</artifactId>-->
+    <!--            <version>5.1.47</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;SpringBoot集成druid连接池&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>com.alibaba</groupId>-->
+    <!--            <artifactId>druid-spring-boot-starter</artifactId>-->
+    <!--            <version>1.1.10</version>-->
+    <!--            <exclusions>-->
+    <!--                <exclusion>-->
+    <!--                    <artifactId>druid</artifactId>-->
+    <!--                    <groupId>com.alibaba</groupId>-->
+    <!--                </exclusion>-->
+    <!--            </exclusions>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>com.alibaba</groupId>-->
+    <!--            <artifactId>druid</artifactId>-->
+    <!--            <version>${druid.version}</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;mybatis和springboot整合&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.mybatis.spring.boot</groupId>-->
+    <!--            <artifactId>mybatis-spring-boot-starter</artifactId>-->
+    <!--            <version>${mybatis.spring.boot.version}</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash; 添加springboot对amqp的支持 &ndash;&gt;-->
+    <!--&lt;!&ndash;        <dependency>&ndash;&gt;-->
+    <!--        &lt;!&ndash;            <groupId>org.springframework.boot</groupId>&ndash;&gt;-->
+    <!--        &lt;!&ndash;            <artifactId>spring-boot-starter-amqp</artifactId>&ndash;&gt;-->
+    <!--        &lt;!&ndash;        </dependency>&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>commons-codec</groupId>-->
+    <!--            <artifactId>commons-codec</artifactId>-->
+    <!--            <version>1.10</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;通用基础配置junit/devtools/test/log4j/lombok/hutool&ndash;&gt;-->
+    <!--        &lt;!&ndash;hutool&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>cn.hutool</groupId>-->
+    <!--            <artifactId>hutool-all</artifactId>-->
+    <!--            <version>5.2.3</version>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>junit</groupId>-->
+    <!--            <artifactId>junit</artifactId>-->
+    <!--            <version>${junit.version}</version>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.springframework.boot</groupId>-->
+    <!--            <artifactId>spring-boot-devtools</artifactId>-->
+    <!--            <scope>runtime</scope>-->
+    <!--            <optional>true</optional>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.springframework.boot</groupId>-->
+    <!--            <artifactId>spring-boot-starter-test</artifactId>-->
+    <!--            <scope>test</scope>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>log4j</groupId>-->
+    <!--            <artifactId>log4j</artifactId>-->
+    <!--            <version>${log4j.version}</version>-->
+    <!--        </dependency>-->
+    <!--        <dependency>-->
+    <!--            <groupId>org.projectlombok</groupId>-->
+    <!--            <artifactId>lombok</artifactId>-->
+    <!--            <version>${lombok.version}</version>-->
+    <!--            <optional>true</optional>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;persistence&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>javax.persistence</groupId>-->
+    <!--            <artifactId>persistence-api</artifactId>-->
+    <!--            <version>1.0.2</version>-->
+    <!--        </dependency>-->
+    <!--        &lt;!&ndash;通用Mapper&ndash;&gt;-->
+    <!--        <dependency>-->
+    <!--            <groupId>tk.mybatis</groupId>-->
+    <!--            <artifactId>mapper</artifactId>-->
+    <!--            <version>${mapper.version}</version>-->
+    <!--            <exclusions>-->
+    <!--                <exclusion>-->
+    <!--                    <artifactId>persistence-api</artifactId>-->
+    <!--                    <groupId>javax.persistence</groupId>-->
+    <!--                </exclusion>-->
+    <!--            </exclusions>-->
+    <!--        </dependency>-->
+    <!--    </dependencies>-->
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.springframework.boot</groupId>
+                <artifactId>spring-boot-maven-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+
+</project>
Index: redis/src/main/resources/mapper/UserMapper.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/resources/mapper/UserMapper.xml	(date 1621184874738)
+++ redis/src/main/resources/mapper/UserMapper.xml	(date 1621184874738)
@@ -0,0 +1,45 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
+<mapper namespace="com.ly.redis.mapper.UserMapper">
+
+    <resultMap id="UserResult" type="com.ly.redis.entity.UserDO">
+        <id property="id" column="id" />
+        <result property="name" column="name" />
+        <result property="accountName" column="accountName" />
+        <result property="email" column="email" />
+        <result property="phoneNumber" column="phoneNumber" />
+        <result property="password" column="password" />
+        <result property="status" column="status" />
+        <result property="updatePassword" column="updatePassword" />
+        <result property="directSupervisor" column="directSupervisor" />
+        <result property="position" column="position" />
+        <result property="wechatPlatformId" column="wechatPlatformId" />
+        <result property="tenantId" column="tenantId" />
+        <result property="gmtCreated" column="gmtCreated" />
+        <result property="gmtModified" column="gmtModified" />
+        <result property="creator" column="creator" />
+        <result property="modifier" column="modifier" />
+        <result property="isDeleted" column="isDeleted" />
+    </resultMap>
+
+    <sql id="userColumns">
+            a.id as "id",
+            a.name as "name",
+            a.account_name as "accountName",
+            a.email as "email",
+            a.phone_number as "phoneNumber",
+            a.password as "password",
+            a.status as "status",
+            a.update_password as "updatePassword",
+            a.direct_supervisor as "directSupervisor",
+            a.position as "position",
+            a.wechat_platform_id as "wechatPlatformId",
+            a.tenant_id as "tenantId",
+            a.gmt_created as "gmtCreated",
+            a.gmt_modified as "gmtModified",
+            a.creator as "creator",
+            a.modifier as "modifier",
+            a.is_deleted as "isDeleted"
+    </sql>
+
+</mapper>
\ No newline at end of file
Index: redis/src/main/java/com/ly/redis/resource/JHSABProductController.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/resource/JHSABProductController.java	(date 1622266641646)
+++ redis/src/main/java/com/ly/redis/resource/JHSABProductController.java	(date 1622266641646)
@@ -0,0 +1,53 @@
+package com.ly.redis.resource;
+
+import com.ly.redis.constants.Constants;
+import com.ly.redis.entity.Product;
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiOperation;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.util.CollectionUtils;
+import org.springframework.web.bind.annotation.RequestMapping;
+import org.springframework.web.bind.annotation.RequestMethod;
+import org.springframework.web.bind.annotation.RestController;
+
+import java.util.List;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-29 1:36 下午
+ *  * @last modify by [luoyong 2021-05-29 1:36 下午]
+ * @Description: JHSABProductController
+ **/
+@RestController
+@Slf4j
+@Api(description = "聚划算商品列表接口AB")
+public class JHSABProductController {
+    @Autowired
+    private RedisTemplate redisTemplate;
+
+    @RequestMapping(value = "/pruduct/findab", method = RequestMethod.GET)
+    @ApiOperation("按照分页和每页显示容量，点击查看AB")
+    public List<Product> findAB(int page, int size) {
+        List<Product> list = null;
+        long start = (page - 1) * size;
+        long end = start + size - 1;
+        try {
+            //采用redis list数据结构的lrange命令实现分页查询
+            list = this.redisTemplate.opsForList().range(Constants.JHS_KEY_A, start, end);
+            if (CollectionUtils.isEmpty(list)) {
+                log.info("=========A缓存已经失效了，记得人工修补，B缓存自动延续5天");
+                //用户先查询缓存A(上面的代码)，如果缓存A查询不到（例如，更新缓存的时候删除了），再查询缓存B
+                this.redisTemplate.opsForList().range(Constants.JHS_KEY_B, start, end);
+            }
+            log.info("查询结果：{}", list);
+        } catch (Exception ex) {
+            //这里的异常，一般是redis瘫痪 ，或 redis网络timeout
+            log.error("exception:", ex);
+            //TODO 走DB查询
+        }
+        return list;
+    }
+
+}
Index: redis/src/main/java/com/ly/redis/constants/Constants.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/constants/Constants.java	(date 1622261181230)
+++ redis/src/main/java/com/ly/redis/constants/Constants.java	(date 1622261181230)
@@ -0,0 +1,18 @@
+package com.ly.redis.constants;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-29 12:05 下午
+ *  * @last modify by [luoyong 2021-05-29 12:05 下午]
+ * @Description: 常量
+ **/
+public class Constants {
+    public static final String JHS_KEY = "jhs";
+
+    public static final String JHS_KEY_A = "jhs:a";
+
+    public static final String JHS_KEY_B = "jhs:b";
+
+    public static final String REBLOOM = "rebloom:uid:";
+
+}
Index: redis/src/main/java/com/ly/redis/resource/HyperLogLogController.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/resource/HyperLogLogController.java	(date 1621667996712)
+++ redis/src/main/java/com/ly/redis/resource/HyperLogLogController.java	(date 1621667996712)
@@ -0,0 +1,33 @@
+package com.ly.redis.resource;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiOperation;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.web.bind.annotation.RequestMapping;
+import org.springframework.web.bind.annotation.RequestMethod;
+import org.springframework.web.bind.annotation.RestController;
+
+import javax.annotation.Resource;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-22 3:19 下午
+ *  * @last modify by [luoyong 2021-05-22 3:19 下午]
+ * @Description: 天猫网站首页亿级UV的Redis统计方案
+ **/
+@Api(description = "天猫网站首页亿级UV的Redis统计方案")
+@RestController
+@Slf4j
+public class HyperLogLogController {
+
+    @Resource
+    private RedisTemplate redisTemplate;
+
+    @ApiOperation("获得ip去重复后的首页访问量，总数统计")
+    @RequestMapping(value = "/uv", method = RequestMethod.GET)
+    public long uv() {
+        //pfcount
+        return redisTemplate.opsForHyperLogLog().size("hll");
+    }
+}
Index: redis/src/main/java/com/ly/redis/resource/JHSProductController.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/resource/JHSProductController.java	(date 1622261712593)
+++ redis/src/main/java/com/ly/redis/resource/JHSProductController.java	(date 1622261712593)
@@ -0,0 +1,54 @@
+package com.ly.redis.resource;
+
+import com.ly.redis.constants.Constants;
+import com.ly.redis.entity.Product;
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiOperation;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.util.CollectionUtils;
+import org.springframework.web.bind.annotation.RequestMapping;
+import org.springframework.web.bind.annotation.RequestMethod;
+import org.springframework.web.bind.annotation.RestController;
+
+import java.util.List;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-29 12:11 下午
+ *  * @last modify by [luoyong 2021-05-29 12:11 下午]
+ * @Description: JHSProductController
+ **/
+@RestController
+@Slf4j
+@Api("聚划算商品列表接口")
+public class JHSProductController {
+    @Autowired
+    RedisTemplate redisTemplate;
+
+    /**
+     * 分页查询：在高并发的情况下，只能走redis查询，走db的话必定会把db打垮
+     * http://localhost:5555/swagger-ui.html#/jhs-product-controller/findUsingGET
+     */
+    @RequestMapping(value = "/pruduct/find",method = RequestMethod.GET)
+    @ApiOperation("按照分页和每页显示容量，点击查看")
+    public List<Product> find(int page, int size) {
+        List<Product> list=null;
+        long start = (page - 1) * size;
+        long end = start + size - 1;
+        try {
+            //采用redis list数据结构的lrange命令实现分页查询
+            list = this.redisTemplate.opsForList().range(Constants.JHS_KEY, start, end);
+            if (CollectionUtils.isEmpty(list)) {
+                //TODO 走DB查询
+            }
+            log.info("查询结果：{}", list);
+        } catch (Exception ex) {
+            //这里的异常，一般是redis瘫痪 ，或 redis网络timeout
+            log.error("exception:", ex);
+            //TODO 走DB查询
+        }
+        return list;
+    }
+}
Index: redis/src/test/java/com/ly/redis/mapper/UserDAOTest.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/test/java/com/ly/redis/mapper/UserDAOTest.java	(date 1621180874153)
+++ redis/src/test/java/com/ly/redis/mapper/UserDAOTest.java	(date 1621180874153)
@@ -0,0 +1,14 @@
+package com.ly.redis.mapper;
+
+/**
+ *
+ *
+ * @author luoyong
+ *  * @create 2021-05-16 8:08 下午
+ *  * @last modify by [luoyong 2021-05-16 8:08 下午]
+ *
+ * @Description: TODO
+ **/
+class UserDAOTest {
+
+}
\ No newline at end of file
Index: redis/src/main/java/com/ly/redis/bloomfilter/RedissonBloomFilterDemo.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/bloomfilter/RedissonBloomFilterDemo.java	(date 1621746347170)
+++ redis/src/main/java/com/ly/redis/bloomfilter/RedissonBloomFilterDemo.java	(date 1621746347170)
@@ -0,0 +1,107 @@
+package com.ly.redis.bloomfilter;
+
+import org.redisson.Redisson;
+import org.redisson.api.RBloomFilter;
+import org.redisson.api.RBucket;
+import org.redisson.api.RedissonClient;
+import org.redisson.client.codec.StringCodec;
+import org.redisson.config.Config;
+import org.springframework.data.redis.core.RedisTemplate;
+
+import javax.annotation.Resource;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-23 12:26 下午
+ *  * @last modify by [luoyong 2021-05-23 12:26 下午]
+ * @Description: Redis布隆过滤器解决缓存穿透
+ **/
+public class RedissonBloomFilterDemo {
+    public static final int _1W = 10000;
+    //布隆过滤器里预计要插入多少数据
+    public static int size = 100 * _1W;
+    //误判率,它越小误判的个数也就越少
+    public static double fpp = 0.03;
+
+
+    //jedis
+    static RedissonClient redissonClient = null;
+
+    //redis版内置的布隆过滤器
+    static RBloomFilter rBloomFilter = null;
+
+    @Resource
+    RedisTemplate redisTemplate;
+
+    public static void main(String[] args) {
+        //String phoneListById = getPhoneListById("10086");
+        //请测试执行2次
+        //String phoneListById = getPhoneListById("10087");
+        String phoneListById = getPhoneListById("10088");
+        System.out.println("------查询出来的结果： " + phoneListById);
+
+        //暂停几秒钟线程
+        try {
+            TimeUnit.SECONDS.sleep(1);
+        } catch (InterruptedException e) {
+            e.printStackTrace();
+        }
+        redissonClient.shutdown();
+    }
+
+
+    static {
+        Config config = new Config();
+        config.useSingleServer().setAddress("redis://192.168.111.147:6379").setDatabase(0);
+        //构造redisson
+        redissonClient = Redisson.create(config);
+        //通过redisson构造rBloomFilter
+        rBloomFilter = redissonClient.getBloomFilter("phoneListBloomFilter", new StringCodec());
+
+        rBloomFilter.tryInit(size, fpp);
+
+        // 1测试  布隆过滤器有+redis有
+        //rBloomFilter.add("10086");
+        //redissonClient.getBucket("10086",new StringCodec()).set("chinamobile10086");
+
+        // 2测试  布隆过滤器有+redis无
+        //rBloomFilter.add("10087");
+
+        //3 测试 ，布隆过滤器无+redis无
+
+    }
+
+
+    private static String getPhoneListById(String IDNumber) {
+        String result = null;
+
+        if (IDNumber == null) {
+            return null;
+        }
+        //1 先去布隆过滤器里面查询
+        if (rBloomFilter.contains(IDNumber)) {
+            //2 布隆过滤器里有，再去redis里面查询
+            RBucket<String> rBucket = redissonClient.getBucket(IDNumber, new StringCodec());
+            result = rBucket.get();
+            if (result != null) {
+                return "i come from redis: " + result;
+            } else {
+                result = getPhoneListByMySQL(IDNumber);
+                if (result == null) {
+                    return null;
+                }
+                // 重新将数据更新回redis
+                redissonClient.getBucket(IDNumber, new StringCodec()).set(result);
+            }
+            return "i come from mysql: " + result;
+        }
+        return result;
+    }
+
+    private static String getPhoneListByMySQL(String IDNumber) {
+        return "chinamobile" + IDNumber;
+    }
+
+
+}
Index: redis/src/main/java/com/ly/redis/bloomfilter/GuavaBloomFilterDemo.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/bloomfilter/GuavaBloomFilterDemo.java	(date 1621746798547)
+++ redis/src/main/java/com/ly/redis/bloomfilter/GuavaBloomFilterDemo.java	(date 1621746798547)
@@ -0,0 +1,76 @@
+package com.ly.redis.bloomfilter;
+
+import com.google.common.hash.BloomFilter;
+import com.google.common.hash.Funnels;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-23 12:25 下午
+ *  * @last modify by [luoyong 2021-05-23 12:25 下午]
+ * @Description: Google布隆过滤器Guava解决缓存穿透
+ **/
+public class GuavaBloomFilterDemo {
+    public static final int _1W = 10000;
+    //布隆过滤器里预计要插入多少数据
+    public static int size = 100 * _1W;
+    //误判率,它越小误判的个数也就越少(思考，是不是可以设置的无限小，没有误判岂不更好)
+    public static double fpp = 0.01;
+
+    public static void main(String[] args) {
+        new GuavaBloomFilterDemo().bloomFilter2();
+    }
+
+    /**
+     * helloWorld入门
+     */
+    public void bloomFilter() {
+        // 创建布隆过滤器对象
+        BloomFilter<Integer> filter = BloomFilter.create(Funnels.integerFunnel(), 100);
+        // 判断指定元素是否存在
+        System.out.println(filter.mightContain(1));
+        System.out.println(filter.mightContain(2));
+        // 将元素添加进布隆过滤器
+        filter.put(1);
+        filter.put(2);
+        System.out.println(filter.mightContain(1));
+        System.out.println(filter.mightContain(2));
+
+    }
+
+
+    /**
+     * 误判率演示+源码分析
+     */
+    public void bloomFilter2() {
+        // 构建布隆过滤器
+        BloomFilter<Integer> bloomFilter = BloomFilter.create(Funnels.integerFunnel(), size, fpp);
+
+        //1 先往布隆过滤器里面插入100万的样本数据
+        for (int i = 0; i < size; i++) {
+            bloomFilter.put(i);
+        }
+        List<Integer> listSample = new ArrayList<>(size);
+        //2 这100万的样本数据，是否都在布隆过滤器里面存在？
+        for (int i = 0; i < size; i++) {
+            if (bloomFilter.mightContain(i)) {
+                listSample.add(i);
+                continue;
+            }
+        }
+        System.out.println("存在的数量：" + listSample.size());
+
+        //3 故意取10万个不在过滤器里的值，看看有多少个会被认为在过滤器里,误判率演示
+        List<Integer> list = new ArrayList<>(10 * _1W);
+
+        for (int i = size + 1; i < size + 100000; i++) {
+            if (bloomFilter.mightContain(i)) {
+                System.out.println(i + "\t" + "被误判了.");
+                list.add(i);
+            }
+        }
+        System.out.println("误判的数量：" + list.size());
+    }
+}
Index: redis/src/main/java/com/ly/redis/service/UserService.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/service/UserService.java	(date 1621178741908)
+++ redis/src/main/java/com/ly/redis/service/UserService.java	(date 1621178741908)
@@ -0,0 +1,63 @@
+package com.ly.redis.service;
+
+import com.ly.redis.dto.UserDTO;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-16 10:55 下午
+ *  * @last modify by [luoyong 2021-05-16 10:55 下午]
+ * @Description:
+ **/
+public interface UserService {
+
+    /**
+     * @param userDTO
+     * @return
+     * @Description: 新增用户信息
+     * @author luoyong
+     * @create 10:59 下午 2021/5/16
+     * @last modify by [LuoYong 10:59 下午 2021/5/16 ]
+     */
+    Boolean addUser(UserDTO userDTO);
+
+    /**
+     * @param userId
+     * @return
+     * @Description: 删除用户信息
+     * @author luoyong
+     * @create 11:00 下午 2021/5/16
+     * @last modify by [LuoYong 11:00 下午 2021/5/16 ]
+     */
+    Boolean delUser(Long userId);
+
+    /**
+     * @param userDTO
+     * @return java.lang.Boolean
+     * @Description: 更新用户信息
+     * @author luoyong
+     * @create 11:01 下午 2021/5/16
+     * @last modify by [LuoYong 11:01 下午 2021/5/16 ]
+     */
+    Boolean updateUser(UserDTO userDTO);
+
+    /**
+     * @param userId
+     * @return
+     * @Description: 根据用户id获取用户信息
+     * @author luoyong
+     * @create 11:01 下午 2021/5/16
+     * @last modify by [LuoYong 11:01 下午 2021/5/16 ]
+     */
+    UserDTO getUserInfo(Long userId);
+
+    /**
+     * @param userId
+     * @return
+     * @Description: 根据用户id获取用户信息
+     * @author luoyong
+     * @create 11:01 下午 2021/5/16
+     * @last modify by [LuoYong 11:01 下午 2021/5/16 ]
+     */
+    UserDTO getUserInfo2(Long userId);
+}
+
Index: redis/src/main/java/com/ly/redis/service/HyperLogLogService.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/service/HyperLogLogService.java	(date 1622265344490)
+++ redis/src/main/java/com/ly/redis/service/HyperLogLogService.java	(date 1622265344490)
@@ -0,0 +1,50 @@
+package com.ly.redis.service;
+
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.stereotype.Service;
+
+import javax.annotation.PostConstruct;
+import javax.annotation.Resource;
+import java.util.Random;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-22 3:20 下午
+ *  * @last modify by [luoyong 2021-05-22 3:20 下午]
+ * @Description: HyperLogLogService
+ **/
+@Service
+@Slf4j
+public class HyperLogLogService {
+
+    @Resource
+    private RedisTemplate redisTemplate;
+
+    /**
+     * 模拟有用户来点击首页，每个用户就是不同的ip，不重复记录，重复不记录
+     */
+//    @PostConstruct
+    public void init() {
+        log.info("------模拟后台有用户点击，每个用户ip不同");
+        //自己启动线程模拟，实际上产不是线程
+        new Thread(() -> {
+            String ip = null;
+            for (int i = 1; i <= 200; i++) {
+                Random random = new Random();
+                ip = random.nextInt(255) + "." + random.nextInt(255) + "." + random.nextInt(255) + "." + random.nextInt(255);
+
+                Long hll = redisTemplate.opsForHyperLogLog().add("hll", ip);
+                log.info("ip={},该ip访问过的次数={}", ip, hll);
+                //暂停3秒钟线程
+                try {
+                    TimeUnit.SECONDS.sleep(3);
+                } catch (InterruptedException e) {
+                    e.printStackTrace();
+                }
+            }
+        }, "t1").start();
+    }
+
+}
Index: redis/src/main/java/com/ly/redis/mapper/UserMapper.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/mapper/UserMapper.java	(date 1621184874741)
+++ redis/src/main/java/com/ly/redis/mapper/UserMapper.java	(date 1621184874741)
@@ -0,0 +1,11 @@
+package com.ly.redis.mapper;
+
+import com.ly.redis.entity.UserDO;
+import tk.mybatis.mapper.common.Mapper;
+
+/**
+ * Created by chenshiyang on 2016.
+ */
+public interface UserMapper extends Mapper<UserDO> {
+
+}
Index: redis/src/main/java/com/ly/redis/resource/GeoController.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/resource/GeoController.java	(date 1621673755896)
+++ redis/src/main/java/com/ly/redis/resource/GeoController.java	(date 1621673755896)
@@ -0,0 +1,99 @@
+package com.ly.redis.resource;
+
+import io.swagger.annotations.ApiOperation;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.data.geo.Distance;
+import org.springframework.data.geo.GeoResults;
+import org.springframework.data.geo.Metrics;
+import org.springframework.data.geo.Point;
+import org.springframework.data.geo.Circle;
+import org.springframework.data.redis.connection.RedisGeoCommands;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.web.bind.annotation.RequestMapping;
+import org.springframework.web.bind.annotation.RequestMethod;
+import org.springframework.web.bind.annotation.RestController;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-22 4:54 下午
+ *  * @last modify by [luoyong 2021-05-22 4:54 下午]
+ * @Description: 附近查找案例
+ **/
+@RestController
+public class GeoController {
+    public  static final String CITY ="city";
+
+    @Autowired
+    private RedisTemplate redisTemplate;
+
+    @ApiOperation("新增天安门故宫长城经纬度")
+    @RequestMapping(value = "/geoadd",method = RequestMethod.POST)
+    public String geoAdd()
+    {
+        Map<String, Point> map= new HashMap<>();
+        map.put("天安门",new Point(116.403963,39.915119));
+        map.put("故宫",new Point(116.403414 ,39.924091));
+        map.put("长城" ,new Point(116.024067,40.362639));
+
+        redisTemplate.opsForGeo().add(CITY,map);
+
+        return map.toString();
+    }
+
+    @ApiOperation("获取地理位置的坐标")
+    @RequestMapping(value = "/geopos",method = RequestMethod.GET)
+    public Point position(String member) {
+        //获取经纬度坐标
+        List<Point> list= this.redisTemplate.opsForGeo().position(CITY,member);
+        return list.get(0);
+    }
+
+    @ApiOperation("geohash算法生成的base32编码值")
+    @RequestMapping(value = "/geohash",method = RequestMethod.GET)
+    public String hash(String member) {
+        //geohash算法生成的base32编码值
+        List<String> list= this.redisTemplate.opsForGeo().hash(CITY,member);
+        return list.get(0);
+    }
+
+    @ApiOperation("计算两个位置之间的距离")
+    @RequestMapping(value = "/geodist",method = RequestMethod.GET)
+    public Distance distance(String member1, String member2) {
+        Distance distance= this.redisTemplate.opsForGeo().distance(CITY,member1,member2, RedisGeoCommands.DistanceUnit.KILOMETERS);
+        return distance;
+    }
+
+    /**
+     * 通过经度，纬度查找附近的
+     * 北京王府井位置116.418017,39.914402,这里为了方便讲课，故意写死
+     */
+    @ApiOperation("通过经度，纬度查找附近的")
+    @RequestMapping(value = "/georadius",method = RequestMethod.GET)
+    public GeoResults radiusByxy() {
+        //这个坐标是北京王府井位置
+        Circle circle = new Circle(116.418017, 39.914402, Metrics.MILES.getMultiplier());
+        //返回50条
+        RedisGeoCommands.GeoRadiusCommandArgs args = RedisGeoCommands.GeoRadiusCommandArgs.newGeoRadiusArgs().includeDistance().includeCoordinates().sortAscending().limit(10);
+        GeoResults<RedisGeoCommands.GeoLocation<String>> geoResults= this.redisTemplate.opsForGeo().radius(CITY,circle, args);
+        return geoResults;
+    }
+
+    /**
+     * 通过地方查找附近
+     */
+    @ApiOperation("通过地方查找附近")
+    @RequestMapping(value = "/georadiusByMember",method = RequestMethod.GET)
+    public GeoResults radiusByMember() {
+        String member="天安门";
+        //返回50条
+        RedisGeoCommands.GeoRadiusCommandArgs args = RedisGeoCommands.GeoRadiusCommandArgs.newGeoRadiusArgs().includeDistance().includeCoordinates().sortAscending().limit(10);
+        //半径10公里内
+        Distance distance=new Distance(10, Metrics.KILOMETERS);
+        GeoResults<RedisGeoCommands.GeoLocation<String>> geoResults= this.redisTemplate.opsForGeo().radius(CITY,member, distance,args);
+        return geoResults;
+    }
+}
Index: boot_redis01/src/main/java/com/atguigu/redis/controller/GoodController.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- boot_redis01/src/main/java/com/atguigu/redis/controller/GoodController.java	(revision daf4ff2ec2eebaa1aaf648cc1fcf5978f14fb9cc)
+++ boot_redis01/src/main/java/com/atguigu/redis/controller/GoodController.java	(date 1622861296795)
@@ -1,5 +1,6 @@
 package com.atguigu.redis.controller;
 
+import com.atguigu.redis.util.RedisUtils;
 import org.redisson.Redisson;
 import org.redisson.api.RLock;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -7,7 +8,9 @@
 import org.springframework.data.redis.core.StringRedisTemplate;
 import org.springframework.web.bind.annotation.GetMapping;
 import org.springframework.web.bind.annotation.RestController;
+import redis.clients.jedis.Jedis;
 
+import java.util.Collections;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.Lock;
@@ -199,7 +202,7 @@
     /**
      * @return java.lang.String
      * @Description: 设置key+过期时间成原子性操作
-     * 问题 张冠李戴 误删
+     * 问题 张冠李戴 误删 A加锁 执行业务逻辑-->过期时间到-->锁释放--->B线程进来--加锁-->A执行完业务逻辑-->删除锁，释放资源（但是这个时候其实是释放B的锁）
      * @author luoyong
      * @create 5:33 下午 2021/5/29
      * @last modify by [LuoYong 5:33 下午 2021/5/29 ]
@@ -231,8 +234,132 @@
     }
 
 
-    @GetMapping("/buy_goods")
-    public String buy_Goods() throws Exception {
+    /**
+     * @return java.lang.String
+     * @Description: 问题：判断和删除之前不是原子性操作-->使用lua脚本解决原子性问题
+     * @author luoyong
+     * @create 10:05 上午 2021/6/5
+     * @last modify by [LuoYong 10:05 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods007")
+    public String buyGoods007() throws Exception {
+        String value = UUID.randomUUID().toString() + Thread.currentThread().getName();
+        try {
+            //设置key+过期时间合成一条命令
+            Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(KEY, value, 10L, TimeUnit.SECONDS);
+            if (!flag) {
+                return "抢锁失败,please try again";
+            }
+
+            String result = stringRedisTemplate.opsForValue().get("goods:001");
+            int goodsNumber = result == null ? 0 : Integer.parseInt(result);
+            if (goodsNumber > 0) {
+                int realNumber = goodsNumber - 1;
+                stringRedisTemplate.opsForValue().set("goods:001", realNumber + "");
+                System.out.println("你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort);
+                return "你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort;
+            } else {
+                System.out.println("商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort);
+            }
+            return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
+        } finally {
+            if (stringRedisTemplate.opsForValue().get(KEY).equals(value)) {
+                stringRedisTemplate.delete(KEY);
+            }
+        }
+    }
+
+
+    /**
+     * @return java.lang.String
+     * @Description: 确保redisLock过期时间大于业务执行时间的问题?---->Redis分布式锁如何续期？
+     * @author luoyong
+     * @create 10:05 上午 2021/6/5
+     * @last modify by [LuoYong 10:05 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods008")
+    public String buyGoods008() throws Exception {
+        String value = UUID.randomUUID().toString() + Thread.currentThread().getName();
+        try {
+            //设置key+过期时间合成一条命令
+            Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(KEY, value, 10L, TimeUnit.SECONDS);
+            if (!flag) {
+                return "抢锁失败,please try again";
+            }
+
+            String result = stringRedisTemplate.opsForValue().get("goods:001");
+            int goodsNumber = result == null ? 0 : Integer.parseInt(result);
+            if (goodsNumber > 0) {
+                int realNumber = goodsNumber - 1;
+                stringRedisTemplate.opsForValue().set("goods:001", realNumber + "");
+                System.out.println("你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort);
+                return "你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort;
+            } else {
+                System.out.println("商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort);
+            }
+            return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
+        } finally {
+            Jedis jedis = RedisUtils.getJedis();
+            String script = "if redis.call('get', KEYS[1]) == ARGV[1] " +
+                    "then " +
+                    "return redis.call('del', KEYS[1]) " +
+                    "else " +
+                    "return 0 " +
+                    "end";
+            try {
+                Object result = jedis.eval(script, Collections.singletonList(KEY), Collections.singletonList(value));
+                if ("1".equals(result.toString())) {
+                    System.out.println("----- del redis_lock_key success");
+                } else {
+                    System.out.println("----- del redis_lock_key fail");
+                }
+            } finally {
+                if (null != jedis) {
+                    jedis.close();
+                }
+            }
+        }
+    }
+
+
+    /**
+     * @return java.lang.String
+     * @Description: 分布式锁 Redisson
+     * @author luoyong
+     * @create 10:44 上午 2021/6/5
+     * @last modify by [LuoYong 10:44 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods009")
+    public String buyGoods009() throws Exception {
+        RLock redissonLock = redisson.getLock(KEY);
+        redissonLock.lock();
+        try {
+
+            String result = stringRedisTemplate.opsForValue().get("goods:001");
+            int goodsNumber = result == null ? 0 : Integer.parseInt(result);
+            if (goodsNumber > 0) {
+                int realNumber = goodsNumber - 1;
+                stringRedisTemplate.opsForValue().set("goods:001", realNumber + "");
+                System.out.println("你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort);
+                return "你已经成功秒杀商品，此时还剩余：" + realNumber + "件" + "\t 服务器端口：" + serverPort;
+            } else {
+                System.out.println("商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort);
+            }
+            return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
+        } finally {
+            redissonLock.unlock();
+        }
+    }
+
+    /**
+     * @return java.lang.String
+     * @Description: 分布式锁 Redisson
+     * @author luoyong
+     * @create 10:44 上午 2021/6/5
+     * @last modify by [LuoYong 10:44 上午 2021/6/5 ]
+     */
+    @GetMapping("/buyGoods010")
+    public String buyGoods010() throws Exception {
         RLock redissonLock = redisson.getLock(KEY);
         redissonLock.lock();
         try {
@@ -250,8 +377,10 @@
             return "商品已经售罄/活动结束/调用超时，欢迎下次光临" + "\t 服务器端口：" + serverPort;
         } finally {
             if (redissonLock.isLocked() && redissonLock.isHeldByCurrentThread()) {
+                //防止太快了 删除了别人的锁
                 redissonLock.unlock();
             }
         }
     }
-}
\ No newline at end of file
+}
+
Index: redis/src/main/java/com/ly/redis/resource/UserResource.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/resource/UserResource.java	(date 1621179150364)
+++ redis/src/main/java/com/ly/redis/resource/UserResource.java	(date 1621179150364)
@@ -0,0 +1,54 @@
+package com.ly.redis.resource;
+
+import com.ly.redis.dto.UserDTO;
+import com.ly.redis.service.UserService;
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiOperation;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.web.bind.annotation.*;
+
+import javax.annotation.Resource;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-16 10:53 下午
+ *  * @last modify by [luoyong 2021-05-16 10:53 下午]
+ * @Description: 用户接口信息
+ **/
+@Api("用户信息")
+@RestController
+@Slf4j
+public class UserResource {
+    @Resource
+    private UserService userService;
+
+    @ApiOperation("数据库新增5条记录")
+    @RequestMapping(value = "/user/add", method = RequestMethod.POST)
+    public Boolean addUser(@RequestBody UserDTO userDTO) {
+        return userService.addUser(userDTO);
+    }
+
+    @ApiOperation("删除用户")
+    @RequestMapping(value = "/user/delete/{id}", method = RequestMethod.POST)
+    public Boolean deleteUser(@PathVariable Long id) {
+        return userService.delUser(id);
+    }
+
+    @ApiOperation("修改用户信息")
+    @RequestMapping(value = "/user/update", method = RequestMethod.POST)
+    public Boolean updateUser(@RequestBody UserDTO userDTO) {
+        return userService.updateUser(userDTO);
+    }
+
+    @ApiOperation("查询用户信息")
+    @RequestMapping(value = "/user/find/{id}", method = RequestMethod.GET)
+    public UserDTO findUserById(@PathVariable Long id) {
+        return userService.getUserInfo(id);
+    }
+
+    @ApiOperation("查询用户信息")
+    @RequestMapping(value = "/user/find/2/{id}", method = RequestMethod.GET)
+    public UserDTO findUserById2(@PathVariable Long id) {
+        return userService.getUserInfo2(id);
+    }
+}
Index: redis/src/main/java/com/ly/redis/service/JHSTaskService.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/service/JHSTaskService.java	(date 1622266562192)
+++ redis/src/main/java/com/ly/redis/service/JHSTaskService.java	(date 1622266562192)
@@ -0,0 +1,77 @@
+package com.ly.redis.service;
+
+import com.ly.redis.constants.Constants;
+import com.ly.redis.entity.Product;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.stereotype.Service;
+
+import javax.annotation.PostConstruct;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-29 11:53 上午
+ *  * @last modify by [luoyong 2021-05-29 11:53 上午]
+ * @Description: 聚划算活动 目前代码适用于单机 多机需要改造代码
+ **/
+@Service
+@Slf4j
+public class JHSTaskService {
+    @Autowired
+    private RedisTemplate redisTemplate;
+
+    /**
+     * @return void
+     * @Description: init
+     * 问题：
+     * 1：redisTemplate.delete与redisTemplate.opsForList().leftPushAll不是原子性操作
+     * @author luoyong
+     * @create 1:25 下午 2021/5/29
+     * @last modify by [LuoYong 1:25 下午 2021/5/29 ]
+     */
+    @PostConstruct
+    public void initJHS() {
+        log.info("启动定时器淘宝聚划算功能模拟");
+        //模拟定时器 定时把mysql数据库的活动商品 刷新到redis当中
+        new Thread(() -> {
+            while (true) {
+                //模拟定时器 从数据库读取100件特价商品 用于加载到聚划算的页面当中
+                List<Product> productList = listProducts();
+                //采用redis list数据结构的lpush来实现存储
+                redisTemplate.delete(Constants.JHS_KEY);
+                //lpush命令
+                redisTemplate.opsForList().leftPushAll(Constants.JHS_KEY, productList);
+                //一分钟执行一次
+                try {
+                    TimeUnit.MINUTES.sleep(1);
+                } catch (InterruptedException e) {
+                    e.printStackTrace();
+                }
+                log.info("runJHS定时刷新............");
+            }
+        }, "JHS").start();
+    }
+
+    /**
+     * @return
+     * @Description: 获取产品信息集合 模拟
+     * @author luoyong
+     * @create 12:01 下午 2021/5/29
+     * @last modify by [LuoYong 12:01 下午 2021/5/29 ]
+     */
+    public List<Product> listProducts() {
+        List<Product> productList = new ArrayList<>();
+        for (int i = 0; i < 20; i++) {
+            Random rand = new Random();
+            int id = rand.nextInt(10000);
+            Product obj = new Product((long) id, "product" + i, i, "detail");
+            productList.add(obj);
+        }
+        return productList;
+    }
+}
Index: boot_redis01/src/main/java/com/atguigu/redis/util/RedisUtils.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- boot_redis01/src/main/java/com/atguigu/redis/util/RedisUtils.java	(revision daf4ff2ec2eebaa1aaf648cc1fcf5978f14fb9cc)
+++ boot_redis01/src/main/java/com/atguigu/redis/util/RedisUtils.java	(date 1622858933837)
@@ -19,7 +19,7 @@
         JedisPoolConfig jedisPoolConfig=new JedisPoolConfig();
         jedisPoolConfig.setMaxTotal(20);
         jedisPoolConfig.setMaxIdle(10);
-        jedisPool=new JedisPool(jedisPoolConfig,"192.168.111.147",6379);
+        jedisPool=new JedisPool(jedisPoolConfig,"192.168.0.122",6379);
     }
 
     public static Jedis getJedis() throws Exception {
Index: redis/src/main/java/com/ly/redis/service/JHSABTaskService.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/service/JHSABTaskService.java	(date 1622266562195)
+++ redis/src/main/java/com/ly/redis/service/JHSABTaskService.java	(date 1622266562195)
@@ -0,0 +1,58 @@
+package com.ly.redis.service;
+
+import cn.hutool.core.date.DateUtil;
+import com.ly.redis.constants.Constants;
+import com.ly.redis.entity.Product;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.stereotype.Service;
+
+import javax.annotation.PostConstruct;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-29 1:34 下午
+ *  * @last modify by [luoyong 2021-05-29 1:34 下午]
+ * @Description: 双缓存机制防止缓存击穿
+ **/
+@Service
+@Slf4j
+public class JHSABTaskService {
+    @Autowired
+    private RedisTemplate redisTemplate;
+
+    @Autowired
+    JHSTaskService jhsTaskService;
+
+    @PostConstruct
+    public void initJHSAB() {
+        log.info("启动AB定时器计划任务淘宝聚划算功能模拟.........." + DateUtil.now());
+        new Thread(() -> {
+            //模拟定时器，定时把数据库的特价商品，刷新到redis中
+            while (true) {
+                //模拟从数据库读取100件特价商品，用于加载到聚划算的页面中
+                List<Product> list = jhsTaskService.listProducts();
+                //先更新B缓存
+                this.redisTemplate.delete(Constants.JHS_KEY_B);
+                this.redisTemplate.opsForList().leftPushAll(Constants.JHS_KEY_B, list);
+                this.redisTemplate.expire(Constants.JHS_KEY_B, 20L, TimeUnit.DAYS);
+                //再更新A缓存
+                this.redisTemplate.delete(Constants.JHS_KEY_A);
+                this.redisTemplate.opsForList().leftPushAll(Constants.JHS_KEY_A, list);
+                this.redisTemplate.expire(Constants.JHS_KEY_A, 15L, TimeUnit.DAYS);
+                //间隔一分钟 执行一遍
+                try {
+                    TimeUnit.MINUTES.sleep(1);
+                } catch (InterruptedException e) {
+                    e.printStackTrace();
+                }
+
+                log.info("runJhs定时刷新..............");
+            }
+        }, "JHSAB").start();
+    }
+
+}
Index: redis/src/main/resources/application.properties
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>NATIVE_TO_ASCII_UTF-8
===================================================================
--- redis/src/main/resources/application.properties	(date 1622265303101)
+++ redis/src/main/resources/application.properties	(date 1622265303101)
@@ -0,0 +1,59 @@
+server.port=5555
+spring.application.name=redis6
+# ========================logging 日志相关的配置=====================
+#系统默认，全局root配置的日志形式，可以注释掉
+logging.level.root=warn
+#开发人员自己设置的包结构，对那个package进行什么级别的日志监控
+logging.level.com.ly.redis=info
+#开发人员自定义日志路径和日志名称
+logging.file.name=/Users/luoyong/alidata1/admin/redis/redis6.log
+#%d{HH:mm:ss.SSS}――日志输出时间
+#%thread――输出日志的进程名字，这在Web应用以及异步任务处理中很有用
+#%-5level――日志级别，并且使用5个字符靠左对齐
+#%logger- ――日志输出者的名字
+#%msg――日志消息
+#%n――平台的换行符
+#logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger- %msg%n
+logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger- %msg%n 
+logging.pattern.file=%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger- %msg%n
+# ========================alibaba.druid相关配置=====================
+spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
+spring.datasource.driver-class-name=com.mysql.jdbc.Driver
+spring.datasource.url=jdbc:mysql://localhost:3306/redis?useUnicode=true&characterEncoding=utf-8&useSSL=false
+spring.datasource.username=root
+spring.datasource.password=root
+spring.datasource.druid.test-while-idle=false
+# ========================redis相关配置=====================
+# Redis数据库索引（默认为0）
+spring.redis.database=0
+# Redis服务器地址
+#spring.redis.host=192.168.111.150
+spring.redis.host=192.168.0.49
+# Redis服务器连接端口
+spring.redis.port=6379
+# Redis服务器连接密码（默认为空）
+spring.redis.password=
+# 连接池最大连接数（使用负值表示没有限制） 默认 8
+spring.redis.lettuce.pool.max-active=8
+# 连接池最大阻塞等待时间（使用负值表示没有限制） 默认 -1，记得加入单位ms，不然idea报红色
+spring.redis.lettuce.pool.max-wait=-1ms
+# 连接池中的最大空闲连接 默认 8
+spring.redis.lettuce.pool.max-idle=8
+# 连接池中的最小空闲连接 默认 0
+spring.redis.lettuce.pool.min-idle=0
+# ========================mybatis相关配置===================
+mybatis.mapper-locations=classpath:mapper/*.xml
+mybatis.type-aliases-package=com.ly.redis.entity
+# ========================swagger=====================
+spring.swagger2.enabled=true
+# ========================rabbitmq相关配置===================
+spring.rabbitmq.host=127.0.0.1
+spring.rabbitmq.port=5672
+spring.rabbitmq.username=guest
+spring.rabbitmq.password=guest
+spring.rabbitmq.virtual-host=/
+# ========================redis 布隆过滤器相关配置=====================
+#redis.bloom.url= 192.168.111.147
+#redis.bloom.port= 6379
+#redis.bloom.init-capacity= 10000
+#redis.bloom.error-rate= 0.01
\ No newline at end of file
Index: redis/src/main/java/com/ly/redis/RedisApplication.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/RedisApplication.java	(date 1621184836497)
+++ redis/src/main/java/com/ly/redis/RedisApplication.java	(date 1621184836497)
@@ -0,0 +1,15 @@
+package com.ly.redis;
+
+import org.springframework.boot.SpringApplication;
+import org.springframework.boot.autoconfigure.SpringBootApplication;
+import tk.mybatis.spring.annotation.MapperScan;
+
+@SpringBootApplication
+@MapperScan("com.ly.redis.mapper")
+public class RedisApplication {
+
+    public static void main(String[] args) {
+        SpringApplication.run(RedisApplication.class, args);
+    }
+
+}
Index: redis/src/main/resources/mybatis-config.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/resources/mybatis-config.xml	(date 1520342646000)
+++ redis/src/main/resources/mybatis-config.xml	(date 1520342646000)
@@ -0,0 +1,10 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!DOCTYPE configuration
+        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
+        "http://mybatis.org/dtd/mybatis-3-config.dtd">
+<configuration>
+
+    <settings>
+        <setting name="mapUnderscoreToCamelCase" value="true"/>
+    </settings>
+</configuration>
\ No newline at end of file
Index: redis/src/test/java/com/ly/redis/RedisApplicationTests.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/test/java/com/ly/redis/RedisApplicationTests.java	(date 1621162769597)
+++ redis/src/test/java/com/ly/redis/RedisApplicationTests.java	(date 1621162769597)
@@ -0,0 +1,13 @@
+package com.ly.redis;
+
+import org.junit.jupiter.api.Test;
+import org.springframework.boot.test.context.SpringBootTest;
+
+@SpringBootTest
+class RedisApplicationTests {
+
+    @Test
+    void contextLoads() {
+    }
+
+}
Index: redis/src/test/a.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/test/a.txt	(date 1621182621879)
+++ redis/src/test/a.txt	(date 1621182621879)
@@ -0,0 +1,2051 @@
+# Redis configuration file example.
+#
+# Note that in order to read the configuration file, Redis must be
+# started with the file path as first argument:
+#
+# ./redis-server /path/to/redis.conf
+
+# Note on units: when memory size is needed, it is possible to specify
+# it in the usual form of 1k 5GB 4M and so forth:
+#
+# 1k => 1000 bytes
+# 1kb => 1024 bytes
+# 1m => 1000000 bytes
+# 1mb => 1024*1024 bytes
+# 1g => 1000000000 bytes
+# 1gb => 1024*1024*1024 bytes
+#
+# units are case insensitive so 1GB 1Gb 1gB are all the same.
+
+################################## INCLUDES ###################################
+
+# Include one or more other config files here.  This is useful if you
+# have a standard template that goes to all Redis servers but also need
+# to customize a few per-server settings.  Include files can include
+# other files, so use this wisely.
+#
+# Note that option "include" won't be rewritten by command "CONFIG REWRITE"
+# from admin or Redis Sentinel. Since Redis always uses the last processed
+# line as value of a configuration directive, you'd better put includes
+# at the beginning of this file to avoid overwriting config change at runtime.
+#
+# If instead you are interested in using includes to override configuration
+# options, it is better to use include as the last line.
+#
+# include /path/to/local.conf
+# include /path/to/other.conf
+
+################################## MODULES #####################################
+
+# Load modules at startup. If the server is not able to load modules
+# it will abort. It is possible to use multiple loadmodule directives.
+#
+# loadmodule /path/to/my_module.so
+# loadmodule /path/to/other_module.so
+
+################################## NETWORK #####################################
+
+# By default, if no "bind" configuration directive is specified, Redis listens
+# for connections from all available network interfaces on the host machine.
+# It is possible to listen to just one or multiple selected interfaces using
+# the "bind" configuration directive, followed by one or more IP addresses.
+# Each address can be prefixed by "-", which means that redis will not fail to
+# start if the address is not available. Being not available only refers to
+# addresses that does not correspond to any network interfece. Addresses that
+# are already in use will always fail, and unsupported protocols will always BE
+# silently skipped.
+#
+# Examples:
+#
+# bind 192.168.1.100 10.0.0.1     # listens on two specific IPv4 addresses
+# bind 127.0.0.1 ::1              # listens on loopback IPv4 and IPv6
+# bind * -::*                     # like the default, all available interfaces
+#
+# ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the
+# internet, binding to all the interfaces is dangerous and will expose the
+# instance to everybody on the internet. So by default we uncomment the
+# following bind directive, that will force Redis to listen only on the
+# IPv4 and IPv6 (if available) loopback interface addresses (this means Redis
+# will only be able to accept client connections from the same host that it is
+# running on).
+#
+# IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES
+# JUST COMMENT OUT THE FOLLOWING LINE.
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#bind 127.0.0.1 -::1
+
+# Protected mode is a layer of security protection, in order to avoid that
+# Redis instances left open on the internet are accessed and exploited.
+#
+# When protected mode is on and if:
+#
+# 1) The server is not binding explicitly to a set of addresses using the
+#    "bind" directive.
+# 2) No password is configured.
+#
+# The server only accepts connections from clients connecting from the
+# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain
+# sockets.
+#
+# By default protected mode is enabled. You should disable it only if
+# you are sure you want clients from other hosts to connect to Redis
+# even if no authentication is configured, nor a specific set of interfaces
+# are explicitly listed using the "bind" directive.
+#protected-mode yes
+protected-mode no
+# Accept connections on the specified port, default is 6379 (IANA #815344).
+# If port 0 is specified Redis will not listen on a TCP socket.
+port 6379
+
+# TCP listen() backlog.
+#
+# In high requests-per-second environments you need a high backlog in order
+# to avoid slow clients connection issues. Note that the Linux kernel
+# will silently truncate it to the value of /proc/sys/net/core/somaxconn so
+# make sure to raise both the value of somaxconn and tcp_max_syn_backlog
+# in order to get the desired effect.
+tcp-backlog 511
+
+# Unix socket.
+#
+# Specify the path for the Unix socket that will be used to listen for
+# incoming connections. There is no default, so Redis will not listen
+# on a unix socket when not specified.
+#
+# unixsocket /run/redis.sock
+# unixsocketperm 700
+
+# Close the connection after a client is idle for N seconds (0 to disable)
+timeout 0
+
+# TCP keepalive.
+#
+# If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence
+# of communication. This is useful for two reasons:
+#
+# 1) Detect dead peers.
+# 2) Force network equipment in the middle to consider the connection to be
+#    alive.
+#
+# On Linux, the specified value (in seconds) is the period used to send ACKs.
+# Note that to close the connection the double of the time is needed.
+# On other kernels the period depends on the kernel configuration.
+#
+# A reasonable value for this option is 300 seconds, which is the new
+# Redis default starting with Redis 3.2.1.
+tcp-keepalive 300
+
+################################# TLS/SSL #####################################
+
+# By default, TLS/SSL is disabled. To enable it, the "tls-port" configuration
+# directive can be used to define TLS-listening ports. To enable TLS on the
+# default port, use:
+#
+# port 0
+# tls-port 6379
+
+# Configure a X.509 certificate and private key to use for authenticating the
+# server to connected clients, masters or cluster peers.  These files should be
+# PEM formatted.
+#
+# tls-cert-file redis.crt
+# tls-key-file redis.key
+#
+# If the key file is encrypted using a passphrase, it can be included here
+# as well.
+#
+# tls-key-file-pass secret
+
+# Normally Redis uses the same certificate for both server functions (accepting
+# connections) and client functions (replicating from a master, establishing
+# cluster bus connections, etc.).
+#
+# Sometimes certificates are issued with attributes that designate them as
+# client-only or server-only certificates. In that case it may be desired to use
+# different certificates for incoming (server) and outgoing (client)
+# connections. To do that, use the following directives:
+#
+# tls-client-cert-file client.crt
+# tls-client-key-file client.key
+#
+# If the key file is encrypted using a passphrase, it can be included here
+# as well.
+#
+# tls-client-key-file-pass secret
+
+# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:
+#
+# tls-dh-params-file redis.dh
+
+# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL
+# clients and peers.  Redis requires an explicit configuration of at least one
+# of these, and will not implicitly use the system wide configuration.
+#
+# tls-ca-cert-file ca.crt
+# tls-ca-cert-dir /etc/ssl/certs
+
+# By default, clients (including replica servers) on a TLS port are required
+# to authenticate using valid client side certificates.
+#
+# If "no" is specified, client certificates are not required and not accepted.
+# If "optional" is specified, client certificates are accepted and must be
+# valid if provided, but are not required.
+#
+# tls-auth-clients no
+# tls-auth-clients optional
+
+# By default, a Redis replica does not attempt to establish a TLS connection
+# with its master.
+#
+# Use the following directive to enable TLS on replication links.
+#
+# tls-replication yes
+
+# By default, the Redis Cluster bus uses a plain TCP connection. To enable
+# TLS for the bus protocol, use the following directive:
+#
+# tls-cluster yes
+
+# By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended
+# that older formally deprecated versions are kept disabled to reduce the attack surface.
+# You can explicitly specify TLS versions to support.
+# Allowed values are case insensitive and include "TLSv1", "TLSv1.1", "TLSv1.2",
+# "TLSv1.3" (OpenSSL >= 1.1.1) or any combination.
+# To enable only TLSv1.2 and TLSv1.3, use:
+#
+# tls-protocols "TLSv1.2 TLSv1.3"
+
+# Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information
+# about the syntax of this string.
+#
+# Note: this configuration applies only to <= TLSv1.2.
+#
+# tls-ciphers DEFAULT:!MEDIUM
+
+# Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more
+# information about the syntax of this string, and specifically for TLSv1.3
+# ciphersuites.
+#
+# tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256
+
+# When choosing a cipher, use the server's preference instead of the client
+# preference. By default, the server follows the client's preference.
+#
+# tls-prefer-server-ciphers yes
+
+# By default, TLS session caching is enabled to allow faster and less expensive
+# reconnections by clients that support it. Use the following directive to disable
+# caching.
+#
+# tls-session-caching no
+
+# Change the default number of TLS sessions cached. A zero value sets the cache
+# to unlimited size. The default size is 20480.
+#
+# tls-session-cache-size 5000
+
+# Change the default timeout of cached TLS sessions. The default timeout is 300
+# seconds.
+#
+# tls-session-cache-timeout 60
+
+################################# GENERAL #####################################
+
+# By default Redis does not run as a daemon. Use 'yes' if you need it.
+# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.
+# When Redis is supervised by upstart or systemd, this parameter has no impact.
+daemonize yes
+
+# If you run Redis from upstart or systemd, Redis can interact with your
+# supervision tree. Options:
+#   supervised no      - no supervision interaction
+#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode
+#                        requires "expect stop" in your upstart job config
+#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET
+#                        on startup, and updating Redis status on a regular
+#                        basis.
+#   supervised auto    - detect upstart or systemd method based on
+#                        UPSTART_JOB or NOTIFY_SOCKET environment variables
+# Note: these supervision methods only signal "process is ready."
+#       They do not enable continuous pings back to your supervisor.
+#
+# The default is "no". To run under upstart/systemd, you can simply uncomment
+# the line below:
+#
+# supervised auto
+
+# If a pid file is specified, Redis writes it where specified at startup
+# and removes it at exit.
+#
+# When the server runs non daemonized, no pid file is created if none is
+# specified in the configuration. When the server is daemonized, the pid file
+# is used even if not specified, defaulting to "/var/run/redis.pid".
+#
+# Creating a pid file is best effort: if Redis is not able to create it
+# nothing bad happens, the server will start and run normally.
+#
+# Note that on modern Linux systems "/run/redis.pid" is more conforming
+# and should be used instead.
+pidfile /var/run/redis_6379.pid
+
+# Specify the server verbosity level.
+# This can be one of:
+# debug (a lot of information, useful for development/testing)
+# verbose (many rarely useful info, but not a mess like the debug level)
+# notice (moderately verbose, what you want in production probably)
+# warning (only very important / critical messages are logged)
+loglevel notice
+
+# Specify the log file name. Also the empty string can be used to force
+# Redis to log on the standard output. Note that if you use standard
+# output for logging but daemonize, logs will be sent to /dev/null
+logfile ""
+
+# To enable logging to the system logger, just set 'syslog-enabled' to yes,
+# and optionally update the other syslog parameters to suit your needs.
+# syslog-enabled no
+
+# Specify the syslog identity.
+# syslog-ident redis
+
+# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.
+# syslog-facility local0
+
+# To disable the built in crash log, which will possibly produce cleaner core
+# dumps when they are needed, uncomment the following:
+#
+# crash-log-enabled no
+
+# To disable the fast memory check that's run as part of the crash log, which
+# will possibly let redis terminate sooner, uncomment the following:
+#
+# crash-memcheck-enabled no
+
+# Set the number of databases. The default database is DB 0, you can select
+# a different one on a per-connection basis using SELECT <dbid> where
+# dbid is a number between 0 and 'databases'-1
+databases 16
+
+# By default Redis shows an ASCII art logo only when started to log to the
+# standard output and if the standard output is a TTY and syslog logging is
+# disabled. Basically this means that normally a logo is displayed only in
+# interactive sessions.
+#
+# However it is possible to force the pre-4.0 behavior and always show a
+# ASCII art logo in startup logs by setting the following option to yes.
+always-show-logo no
+
+# By default, Redis modifies the process title (as seen in 'top' and 'ps') to
+# provide some runtime information. It is possible to disable this and leave
+# the process name as executed by setting the following to no.
+set-proc-title yes
+
+# When changing the process title, Redis uses the following template to construct
+# the modified title.
+#
+# Template variables are specified in curly brackets. The following variables are
+# supported:
+#
+# {title}           Name of process as executed if parent, or type of child process.
+# {listen-addr}     Bind address or '*' followed by TCP or TLS port listening on, or
+#                   Unix socket if only that's available.
+# {server-mode}     Special mode, i.e. "[sentinel]" or "[cluster]".
+# {port}            TCP port listening on, or 0.
+# {tls-port}        TLS port listening on, or 0.
+# {unixsocket}      Unix domain socket listening on, or "".
+# {config-file}     Name of configuration file used.
+#
+proc-title-template "{title} {listen-addr} {server-mode}"
+
+################################ SNAPSHOTTING  ################################
+
+# Save the DB to disk.
+#
+# save <seconds> <changes>
+#
+# Redis will save the DB if both the given number of seconds and the given
+# number of write operations against the DB occurred.
+#
+# Snapshotting can be completely disabled with a single empty string argument
+# as in following example:
+#
+# save ""
+#
+# Unless specified otherwise, by default Redis will save the DB:
+#   * After 3600 seconds (an hour) if at least 1 key changed
+#   * After 300 seconds (5 minutes) if at least 100 keys changed
+#   * After 60 seconds if at least 10000 keys changed
+#
+# You can set these explicitly by uncommenting the three following lines.
+#
+# save 3600 1
+# save 300 100
+# save 60 10000
+
+# By default Redis will stop accepting writes if RDB snapshots are enabled
+# (at least one save point) and the latest background save failed.
+# This will make the user aware (in a hard way) that data is not persisting
+# on disk properly, otherwise chances are that no one will notice and some
+# disaster will happen.
+#
+# If the background saving process will start working again Redis will
+# automatically allow writes again.
+#
+# However if you have setup your proper monitoring of the Redis server
+# and persistence, you may want to disable this feature so that Redis will
+# continue to work as usual even if there are problems with disk,
+# permissions, and so forth.
+stop-writes-on-bgsave-error yes
+
+# Compress string objects using LZF when dump .rdb databases?
+# By default compression is enabled as it's almost always a win.
+# If you want to save some CPU in the saving child set it to 'no' but
+# the dataset will likely be bigger if you have compressible values or keys.
+rdbcompression yes
+
+# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.
+# This makes the format more resistant to corruption but there is a performance
+# hit to pay (around 10%) when saving and loading RDB files, so you can disable it
+# for maximum performances.
+#
+# RDB files created with checksum disabled have a checksum of zero that will
+# tell the loading code to skip the check.
+rdbchecksum yes
+
+# Enables or disables full sanitation checks for ziplist and listpack etc when
+# loading an RDB or RESTORE payload. This reduces the chances of a assertion or
+# crash later on while processing commands.
+# Options:
+#   no         - Never perform full sanitation
+#   yes        - Always perform full sanitation
+#   clients    - Perform full sanitation only for user connections.
+#                Excludes: RDB files, RESTORE commands received from the master
+#                connection, and client connections which have the
+#                skip-sanitize-payload ACL flag.
+# The default should be 'clients' but since it currently affects cluster
+# resharding via MIGRATE, it is temporarily set to 'no' by default.
+#
+# sanitize-dump-payload no
+
+# The filename where to dump the DB
+dbfilename dump.rdb
+
+# Remove RDB files used by replication in instances without persistence
+# enabled. By default this option is disabled, however there are environments
+# where for regulations or other security concerns, RDB files persisted on
+# disk by masters in order to feed replicas, or stored on disk by replicas
+# in order to load them for the initial synchronization, should be deleted
+# ASAP. Note that this option ONLY WORKS in instances that have both AOF
+# and RDB persistence disabled, otherwise is completely ignored.
+#
+# An alternative (and sometimes better) way to obtain the same effect is
+# to use diskless replication on both master and replicas instances. However
+# in the case of replicas, diskless is not always an option.
+rdb-del-sync-files no
+
+# The working directory.
+#
+# The DB will be written inside this directory, with the filename specified
+# above using the 'dbfilename' configuration directive.
+#
+# The Append Only File will also be created inside this directory.
+#
+# Note that you must specify a directory here, not a file name.
+dir ./
+
+################################# REPLICATION #################################
+
+# Master-Replica replication. Use replicaof to make a Redis instance a copy of
+# another Redis server. A few things to understand ASAP about Redis replication.
+#
+#   +------------------+      +---------------+
+#   |      Master      | ---> |    Replica    |
+#   | (receive writes) |      |  (exact copy) |
+#   +------------------+      +---------------+
+#
+# 1) Redis replication is asynchronous, but you can configure a master to
+#    stop accepting writes if it appears to be not connected with at least
+#    a given number of replicas.
+# 2) Redis replicas are able to perform a partial resynchronization with the
+#    master if the replication link is lost for a relatively small amount of
+#    time. You may want to configure the replication backlog size (see the next
+#    sections of this file) with a sensible value depending on your needs.
+# 3) Replication is automatic and does not need user intervention. After a
+#    network partition replicas automatically try to reconnect to masters
+#    and resynchronize with them.
+#
+# replicaof <masterip> <masterport>
+
+# If the master is password protected (using the "requirepass" configuration
+# directive below) it is possible to tell the replica to authenticate before
+# starting the replication synchronization process, otherwise the master will
+# refuse the replica request.
+#
+# masterauth <master-password>
+#
+# However this is not enough if you are using Redis ACLs (for Redis version
+# 6 or greater), and the default user is not capable of running the PSYNC
+# command and/or other commands needed for replication. In this case it's
+# better to configure a special user to use with replication, and specify the
+# masteruser configuration as such:
+#
+# masteruser <username>
+#
+# When masteruser is specified, the replica will authenticate against its
+# master using the new AUTH form: AUTH <username> <password>.
+
+# When a replica loses its connection with the master, or when the replication
+# is still in progress, the replica can act in two different ways:
+#
+# 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will
+#    still reply to client requests, possibly with out of date data, or the
+#    data set may just be empty if this is the first synchronization.
+#
+# 2) If replica-serve-stale-data is set to 'no' the replica will reply with
+#    an error "SYNC with master in progress" to all commands except:
+#    INFO, REPLICAOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, SUBSCRIBE,
+#    UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, COMMAND, POST,
+#    HOST and LATENCY.
+#
+replica-serve-stale-data yes
+
+# You can configure a replica instance to accept writes or not. Writing against
+# a replica instance may be useful to store some ephemeral data (because data
+# written on a replica will be easily deleted after resync with the master) but
+# may also cause problems if clients are writing to it because of a
+# misconfiguration.
+#
+# Since Redis 2.6 by default replicas are read-only.
+#
+# Note: read only replicas are not designed to be exposed to untrusted clients
+# on the internet. It's just a protection layer against misuse of the instance.
+# Still a read only replica exports by default all the administrative commands
+# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve
+# security of read only replicas using 'rename-command' to shadow all the
+# administrative / dangerous commands.
+replica-read-only yes
+
+# Replication SYNC strategy: disk or socket.
+#
+# New replicas and reconnecting replicas that are not able to continue the
+# replication process just receiving differences, need to do what is called a
+# "full synchronization". An RDB file is transmitted from the master to the
+# replicas.
+#
+# The transmission can happen in two different ways:
+#
+# 1) Disk-backed: The Redis master creates a new process that writes the RDB
+#                 file on disk. Later the file is transferred by the parent
+#                 process to the replicas incrementally.
+# 2) Diskless: The Redis master creates a new process that directly writes the
+#              RDB file to replica sockets, without touching the disk at all.
+#
+# With disk-backed replication, while the RDB file is generated, more replicas
+# can be queued and served with the RDB file as soon as the current child
+# producing the RDB file finishes its work. With diskless replication instead
+# once the transfer starts, new replicas arriving will be queued and a new
+# transfer will start when the current one terminates.
+#
+# When diskless replication is used, the master waits a configurable amount of
+# time (in seconds) before starting the transfer in the hope that multiple
+# replicas will arrive and the transfer can be parallelized.
+#
+# With slow disks and fast (large bandwidth) networks, diskless replication
+# works better.
+repl-diskless-sync no
+
+# When diskless replication is enabled, it is possible to configure the delay
+# the server waits in order to spawn the child that transfers the RDB via socket
+# to the replicas.
+#
+# This is important since once the transfer starts, it is not possible to serve
+# new replicas arriving, that will be queued for the next RDB transfer, so the
+# server waits a delay in order to let more replicas arrive.
+#
+# The delay is specified in seconds, and by default is 5 seconds. To disable
+# it entirely just set it to 0 seconds and the transfer will start ASAP.
+repl-diskless-sync-delay 5
+
+# -----------------------------------------------------------------------------
+# WARNING: RDB diskless load is experimental. Since in this setup the replica
+# does not immediately store an RDB on disk, it may cause data loss during
+# failovers. RDB diskless load + Redis modules not handling I/O reads may also
+# cause Redis to abort in case of I/O errors during the initial synchronization
+# stage with the master. Use only if you know what you are doing.
+# -----------------------------------------------------------------------------
+#
+# Replica can load the RDB it reads from the replication link directly from the
+# socket, or store the RDB to a file and read that file after it was completely
+# received from the master.
+#
+# In many cases the disk is slower than the network, and storing and loading
+# the RDB file may increase replication time (and even increase the master's
+# Copy on Write memory and salve buffers).
+# However, parsing the RDB file directly from the socket may mean that we have
+# to flush the contents of the current database before the full rdb was
+# received. For this reason we have the following options:
+#
+# "disabled"    - Don't use diskless load (store the rdb file to the disk first)
+# "on-empty-db" - Use diskless load only when it is completely safe.
+# "swapdb"      - Keep a copy of the current db contents in RAM while parsing
+#                 the data directly from the socket. note that this requires
+#                 sufficient memory, if you don't have it, you risk an OOM kill.
+repl-diskless-load disabled
+
+# Replicas send PINGs to server in a predefined interval. It's possible to
+# change this interval with the repl_ping_replica_period option. The default
+# value is 10 seconds.
+#
+# repl-ping-replica-period 10
+
+# The following option sets the replication timeout for:
+#
+# 1) Bulk transfer I/O during SYNC, from the point of view of replica.
+# 2) Master timeout from the point of view of replicas (data, pings).
+# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).
+#
+# It is important to make sure that this value is greater than the value
+# specified for repl-ping-replica-period otherwise a timeout will be detected
+# every time there is low traffic between the master and the replica. The default
+# value is 60 seconds.
+#
+# repl-timeout 60
+
+# Disable TCP_NODELAY on the replica socket after SYNC?
+#
+# If you select "yes" Redis will use a smaller number of TCP packets and
+# less bandwidth to send data to replicas. But this can add a delay for
+# the data to appear on the replica side, up to 40 milliseconds with
+# Linux kernels using a default configuration.
+#
+# If you select "no" the delay for data to appear on the replica side will
+# be reduced but more bandwidth will be used for replication.
+#
+# By default we optimize for low latency, but in very high traffic conditions
+# or when the master and replicas are many hops away, turning this to "yes" may
+# be a good idea.
+repl-disable-tcp-nodelay no
+
+# Set the replication backlog size. The backlog is a buffer that accumulates
+# replica data when replicas are disconnected for some time, so that when a
+# replica wants to reconnect again, often a full resync is not needed, but a
+# partial resync is enough, just passing the portion of data the replica
+# missed while disconnected.
+#
+# The bigger the replication backlog, the longer the replica can endure the
+# disconnect and later be able to perform a partial resynchronization.
+#
+# The backlog is only allocated if there is at least one replica connected.
+#
+# repl-backlog-size 1mb
+
+# After a master has no connected replicas for some time, the backlog will be
+# freed. The following option configures the amount of seconds that need to
+# elapse, starting from the time the last replica disconnected, for the backlog
+# buffer to be freed.
+#
+# Note that replicas never free the backlog for timeout, since they may be
+# promoted to masters later, and should be able to correctly "partially
+# resynchronize" with other replicas: hence they should always accumulate backlog.
+#
+# A value of 0 means to never release the backlog.
+#
+# repl-backlog-ttl 3600
+
+# The replica priority is an integer number published by Redis in the INFO
+# output. It is used by Redis Sentinel in order to select a replica to promote
+# into a master if the master is no longer working correctly.
+#
+# A replica with a low priority number is considered better for promotion, so
+# for instance if there are three replicas with priority 10, 100, 25 Sentinel
+# will pick the one with priority 10, that is the lowest.
+#
+# However a special priority of 0 marks the replica as not able to perform the
+# role of master, so a replica with priority of 0 will never be selected by
+# Redis Sentinel for promotion.
+#
+# By default the priority is 100.
+replica-priority 100
+
+# -----------------------------------------------------------------------------
+# By default, Redis Sentinel includes all replicas in its reports. A replica
+# can be excluded from Redis Sentinel's announcements. An unannounced replica
+# will be ignored by the 'sentinel replicas <master>' command and won't be
+# exposed to Redis Sentinel's clients.
+#
+# This option does not change the behavior of replica-priority. Even with
+# replica-announced set to 'no', the replica can be promoted to master. To
+# prevent this behavior, set replica-priority to 0.
+#
+# replica-announced yes
+
+# It is possible for a master to stop accepting writes if there are less than
+# N replicas connected, having a lag less or equal than M seconds.
+#
+# The N replicas need to be in "online" state.
+#
+# The lag in seconds, that must be <= the specified value, is calculated from
+# the last ping received from the replica, that is usually sent every second.
+#
+# This option does not GUARANTEE that N replicas will accept the write, but
+# will limit the window of exposure for lost writes in case not enough replicas
+# are available, to the specified number of seconds.
+#
+# For example to require at least 3 replicas with a lag <= 10 seconds use:
+#
+# min-replicas-to-write 3
+# min-replicas-max-lag 10
+#
+# Setting one or the other to 0 disables the feature.
+#
+# By default min-replicas-to-write is set to 0 (feature disabled) and
+# min-replicas-max-lag is set to 10.
+
+# A Redis master is able to list the address and port of the attached
+# replicas in different ways. For example the "INFO replication" section
+# offers this information, which is used, among other tools, by
+# Redis Sentinel in order to discover replica instances.
+# Another place where this info is available is in the output of the
+# "ROLE" command of a master.
+#
+# The listed IP address and port normally reported by a replica is
+# obtained in the following way:
+#
+#   IP: The address is auto detected by checking the peer address
+#   of the socket used by the replica to connect with the master.
+#
+#   Port: The port is communicated by the replica during the replication
+#   handshake, and is normally the port that the replica is using to
+#   listen for connections.
+#
+# However when port forwarding or Network Address Translation (NAT) is
+# used, the replica may actually be reachable via different IP and port
+# pairs. The following two options can be used by a replica in order to
+# report to its master a specific set of IP and port, so that both INFO
+# and ROLE will report those values.
+#
+# There is no need to use both the options if you need to override just
+# the port or the IP address.
+#
+# replica-announce-ip 5.5.5.5
+# replica-announce-port 1234
+
+############################### KEYS TRACKING #################################
+
+# Redis implements server assisted support for client side caching of values.
+# This is implemented using an invalidation table that remembers, using
+# a radix key indexed by key name, what clients have which keys. In turn
+# this is used in order to send invalidation messages to clients. Please
+# check this page to understand more about the feature:
+#
+#   https://redis.io/topics/client-side-caching
+#
+# When tracking is enabled for a client, all the read only queries are assumed
+# to be cached: this will force Redis to store information in the invalidation
+# table. When keys are modified, such information is flushed away, and
+# invalidation messages are sent to the clients. However if the workload is
+# heavily dominated by reads, Redis could use more and more memory in order
+# to track the keys fetched by many clients.
+#
+# For this reason it is possible to configure a maximum fill value for the
+# invalidation table. By default it is set to 1M of keys, and once this limit
+# is reached, Redis will start to evict keys in the invalidation table
+# even if they were not modified, just to reclaim memory: this will in turn
+# force the clients to invalidate the cached values. Basically the table
+# maximum size is a trade off between the memory you want to spend server
+# side to track information about who cached what, and the ability of clients
+# to retain cached objects in memory.
+#
+# If you set the value to 0, it means there are no limits, and Redis will
+# retain as many keys as needed in the invalidation table.
+# In the "stats" INFO section, you can find information about the number of
+# keys in the invalidation table at every given moment.
+#
+# Note: when key tracking is used in broadcasting mode, no memory is used
+# in the server side so this setting is useless.
+#
+# tracking-table-max-keys 1000000
+
+################################## SECURITY ###################################
+
+# Warning: since Redis is pretty fast, an outside user can try up to
+# 1 million passwords per second against a modern box. This means that you
+# should use very strong passwords, otherwise they will be very easy to break.
+# Note that because the password is really a shared secret between the client
+# and the server, and should not be memorized by any human, the password
+# can be easily a long string from /dev/urandom or whatever, so by using a
+# long and unguessable password no brute force attack will be possible.
+
+# Redis ACL users are defined in the following format:
+#
+#   user <username> ... acl rules ...
+#
+# For example:
+#
+#   user worker +@list +@connection ~jobs:* on >ffa9203c493aa99
+#
+# The special username "default" is used for new connections. If this user
+# has the "nopass" rule, then new connections will be immediately authenticated
+# as the "default" user without the need of any password provided via the
+# AUTH command. Otherwise if the "default" user is not flagged with "nopass"
+# the connections will start in not authenticated state, and will require
+# AUTH (or the HELLO command AUTH option) in order to be authenticated and
+# start to work.
+#
+# The ACL rules that describe what a user can do are the following:
+#
+#  on           Enable the user: it is possible to authenticate as this user.
+#  off          Disable the user: it's no longer possible to authenticate
+#               with this user, however the already authenticated connections
+#               will still work.
+#  skip-sanitize-payload    RESTORE dump-payload sanitation is skipped.
+#  sanitize-payload         RESTORE dump-payload is sanitized (default).
+#  +<command>   Allow the execution of that command
+#  -<command>   Disallow the execution of that command
+#  +@<category> Allow the execution of all the commands in such category
+#               with valid categories are like @admin, @set, @sortedset, ...
+#               and so forth, see the full list in the server.c file where
+#               the Redis command table is described and defined.
+#               The special category @all means all the commands, but currently
+#               present in the server, and that will be loaded in the future
+#               via modules.
+#  +<command>|subcommand    Allow a specific subcommand of an otherwise
+#                           disabled command. Note that this form is not
+#                           allowed as negative like -DEBUG|SEGFAULT, but
+#                           only additive starting with "+".
+#  allcommands  Alias for +@all. Note that it implies the ability to execute
+#               all the future commands loaded via the modules system.
+#  nocommands   Alias for -@all.
+#  ~<pattern>   Add a pattern of keys that can be mentioned as part of
+#               commands. For instance ~* allows all the keys. The pattern
+#               is a glob-style pattern like the one of KEYS.
+#               It is possible to specify multiple patterns.
+#  allkeys      Alias for ~*
+#  resetkeys    Flush the list of allowed keys patterns.
+#  &<pattern>   Add a glob-style pattern of Pub/Sub channels that can be
+#               accessed by the user. It is possible to specify multiple channel
+#               patterns.
+#  allchannels  Alias for &*
+#  resetchannels            Flush the list of allowed channel patterns.
+#  ><password>  Add this password to the list of valid password for the user.
+#               For example >mypass will add "mypass" to the list.
+#               This directive clears the "nopass" flag (see later).
+#  <<password>  Remove this password from the list of valid passwords.
+#  nopass       All the set passwords of the user are removed, and the user
+#               is flagged as requiring no password: it means that every
+#               password will work against this user. If this directive is
+#               used for the default user, every new connection will be
+#               immediately authenticated with the default user without
+#               any explicit AUTH command required. Note that the "resetpass"
+#               directive will clear this condition.
+#  resetpass    Flush the list of allowed passwords. Moreover removes the
+#               "nopass" status. After "resetpass" the user has no associated
+#               passwords and there is no way to authenticate without adding
+#               some password (or setting it as "nopass" later).
+#  reset        Performs the following actions: resetpass, resetkeys, off,
+#               -@all. The user returns to the same state it has immediately
+#               after its creation.
+#
+# ACL rules can be specified in any order: for instance you can start with
+# passwords, then flags, or key patterns. However note that the additive
+# and subtractive rules will CHANGE MEANING depending on the ordering.
+# For instance see the following example:
+#
+#   user alice on +@all -DEBUG ~* >somepassword
+#
+# This will allow "alice" to use all the commands with the exception of the
+# DEBUG command, since +@all added all the commands to the set of the commands
+# alice can use, and later DEBUG was removed. However if we invert the order
+# of two ACL rules the result will be different:
+#
+#   user alice on -DEBUG +@all ~* >somepassword
+#
+# Now DEBUG was removed when alice had yet no commands in the set of allowed
+# commands, later all the commands are added, so the user will be able to
+# execute everything.
+#
+# Basically ACL rules are processed left-to-right.
+#
+# For more information about ACL configuration please refer to
+# the Redis web site at https://redis.io/topics/acl
+
+# ACL LOG
+#
+# The ACL Log tracks failed commands and authentication events associated
+# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked
+# by ACLs. The ACL Log is stored in memory. You can reclaim memory with
+# ACL LOG RESET. Define the maximum entry length of the ACL Log below.
+acllog-max-len 128
+
+# Using an external ACL file
+#
+# Instead of configuring users here in this file, it is possible to use
+# a stand-alone file just listing users. The two methods cannot be mixed:
+# if you configure users here and at the same time you activate the external
+# ACL file, the server will refuse to start.
+#
+# The format of the external ACL user file is exactly the same as the
+# format that is used inside redis.conf to describe users.
+#
+# aclfile /etc/redis/users.acl
+
+# IMPORTANT NOTE: starting with Redis 6 "requirepass" is just a compatibility
+# layer on top of the new ACL system. The option effect will be just setting
+# the password for the default user. Clients will still authenticate using
+# AUTH <password> as usually, or more explicitly with AUTH default <password>
+# if they follow the new protocol: both will work.
+#
+# The requirepass is not compatable with aclfile option and the ACL LOAD
+# command, these will cause requirepass to be ignored.
+#
+# requirepass foobared
+
+# New users are initialized with restrictive permissions by default, via the
+# equivalent of this ACL rule 'off resetkeys -@all'. Starting with Redis 6.2, it
+# is possible to manage access to Pub/Sub channels with ACL rules as well. The
+# default Pub/Sub channels permission if new users is controlled by the
+# acl-pubsub-default configuration directive, which accepts one of these values:
+#
+# allchannels: grants access to all Pub/Sub channels
+# resetchannels: revokes access to all Pub/Sub channels
+#
+# To ensure backward compatibility while upgrading Redis 6.0, acl-pubsub-default
+# defaults to the 'allchannels' permission.
+#
+# Future compatibility note: it is very likely that in a future version of Redis
+# the directive's default of 'allchannels' will be changed to 'resetchannels' in
+# order to provide better out-of-the-box Pub/Sub security. Therefore, it is
+# recommended that you explicitly define Pub/Sub permissions for all users
+# rather then rely on implicit default values. Once you've set explicit
+# Pub/Sub for all existing users, you should uncomment the following line.
+#
+# acl-pubsub-default resetchannels
+
+# Command renaming (DEPRECATED).
+#
+# ------------------------------------------------------------------------
+# WARNING: avoid using this option if possible. Instead use ACLs to remove
+# commands from the default user, and put them only in some admin user you
+# create for administrative purposes.
+# ------------------------------------------------------------------------
+#
+# It is possible to change the name of dangerous commands in a shared
+# environment. For instance the CONFIG command may be renamed into something
+# hard to guess so that it will still be available for internal-use tools
+# but not available for general clients.
+#
+# Example:
+#
+# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
+#
+# It is also possible to completely kill a command by renaming it into
+# an empty string:
+#
+# rename-command CONFIG ""
+#
+# Please note that changing the name of commands that are logged into the
+# AOF file or transmitted to replicas may cause problems.
+
+################################### CLIENTS ####################################
+
+# Set the max number of connected clients at the same time. By default
+# this limit is set to 10000 clients, however if the Redis server is not
+# able to configure the process file limit to allow for the specified limit
+# the max number of allowed clients is set to the current file limit
+# minus 32 (as Redis reserves a few file descriptors for internal uses).
+#
+# Once the limit is reached Redis will close all the new connections sending
+# an error 'max number of clients reached'.
+#
+# IMPORTANT: When Redis Cluster is used, the max number of connections is also
+# shared with the cluster bus: every node in the cluster will use two
+# connections, one incoming and another outgoing. It is important to size the
+# limit accordingly in case of very large clusters.
+#
+# maxclients 10000
+
+############################## MEMORY MANAGEMENT ################################
+
+# Set a memory usage limit to the specified amount of bytes.
+# When the memory limit is reached Redis will try to remove keys
+# according to the eviction policy selected (see maxmemory-policy).
+#
+# If Redis can't remove keys according to the policy, or if the policy is
+# set to 'noeviction', Redis will start to reply with errors to commands
+# that would use more memory, like SET, LPUSH, and so on, and will continue
+# to reply to read-only commands like GET.
+#
+# This option is usually useful when using Redis as an LRU or LFU cache, or to
+# set a hard memory limit for an instance (using the 'noeviction' policy).
+#
+# WARNING: If you have replicas attached to an instance with maxmemory on,
+# the size of the output buffers needed to feed the replicas are subtracted
+# from the used memory count, so that network problems / resyncs will
+# not trigger a loop where keys are evicted, and in turn the output
+# buffer of replicas is full with DELs of keys evicted triggering the deletion
+# of more keys, and so forth until the database is completely emptied.
+#
+# In short... if you have replicas attached it is suggested that you set a lower
+# limit for maxmemory so that there is some free RAM on the system for replica
+# output buffers (but this is not needed if the policy is 'noeviction').
+#
+# maxmemory <bytes>
+
+# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory
+# is reached. You can select one from the following behaviors:
+#
+# volatile-lru -> Evict using approximated LRU, only keys with an expire set.
+# allkeys-lru -> Evict any key using approximated LRU.
+# volatile-lfu -> Evict using approximated LFU, only keys with an expire set.
+# allkeys-lfu -> Evict any key using approximated LFU.
+# volatile-random -> Remove a random key having an expire set.
+# allkeys-random -> Remove a random key, any key.
+# volatile-ttl -> Remove the key with the nearest expire time (minor TTL)
+# noeviction -> Don't evict anything, just return an error on write operations.
+#
+# LRU means Least Recently Used
+# LFU means Least Frequently Used
+#
+# Both LRU, LFU and volatile-ttl are implemented using approximated
+# randomized algorithms.
+#
+# Note: with any of the above policies, when there are no suitable keys for
+# eviction, Redis will return an error on write operations that require
+# more memory. These are usually commands that create new keys, add data or
+# modify existing keys. A few examples are: SET, INCR, HSET, LPUSH, SUNIONSTORE,
+# SORT (due to the STORE argument), and EXEC (if the transaction includes any
+# command that requires memory).
+#
+# The default is:
+#
+# maxmemory-policy noeviction
+
+# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated
+# algorithms (in order to save memory), so you can tune it for speed or
+# accuracy. By default Redis will check five keys and pick the one that was
+# used least recently, you can change the sample size using the following
+# configuration directive.
+#
+# The default of 5 produces good enough results. 10 Approximates very closely
+# true LRU but costs more CPU. 3 is faster but not very accurate.
+#
+# maxmemory-samples 5
+
+# Eviction processing is designed to function well with the default setting.
+# If there is an unusually large amount of write traffic, this value may need to
+# be increased.  Decreasing this value may reduce latency at the risk of
+# eviction processing effectiveness
+#   0 = minimum latency, 10 = default, 100 = process without regard to latency
+#
+# maxmemory-eviction-tenacity 10
+
+# Starting from Redis 5, by default a replica will ignore its maxmemory setting
+# (unless it is promoted to master after a failover or manually). It means
+# that the eviction of keys will be just handled by the master, sending the
+# DEL commands to the replica as keys evict in the master side.
+#
+# This behavior ensures that masters and replicas stay consistent, and is usually
+# what you want, however if your replica is writable, or you want the replica
+# to have a different memory setting, and you are sure all the writes performed
+# to the replica are idempotent, then you may change this default (but be sure
+# to understand what you are doing).
+#
+# Note that since the replica by default does not evict, it may end using more
+# memory than the one set via maxmemory (there are certain buffers that may
+# be larger on the replica, or data structures may sometimes take more memory
+# and so forth). So make sure you monitor your replicas and make sure they
+# have enough memory to never hit a real out-of-memory condition before the
+# master hits the configured maxmemory setting.
+#
+# replica-ignore-maxmemory yes
+
+# Redis reclaims expired keys in two ways: upon access when those keys are
+# found to be expired, and also in background, in what is called the
+# "active expire key". The key space is slowly and interactively scanned
+# looking for expired keys to reclaim, so that it is possible to free memory
+# of keys that are expired and will never be accessed again in a short time.
+#
+# The default effort of the expire cycle will try to avoid having more than
+# ten percent of expired keys still in memory, and will try to avoid consuming
+# more than 25% of total memory and to add latency to the system. However
+# it is possible to increase the expire "effort" that is normally set to
+# "1", to a greater value, up to the value "10". At its maximum value the
+# system will use more CPU, longer cycles (and technically may introduce
+# more latency), and will tolerate less already expired keys still present
+# in the system. It's a tradeoff between memory, CPU and latency.
+#
+# active-expire-effort 1
+
+############################# LAZY FREEING ####################################
+
+# Redis has two primitives to delete keys. One is called DEL and is a blocking
+# deletion of the object. It means that the server stops processing new commands
+# in order to reclaim all the memory associated with an object in a synchronous
+# way. If the key deleted is associated with a small object, the time needed
+# in order to execute the DEL command is very small and comparable to most other
+# O(1) or O(log_N) commands in Redis. However if the key is associated with an
+# aggregated value containing millions of elements, the server can block for
+# a long time (even seconds) in order to complete the operation.
+#
+# For the above reasons Redis also offers non blocking deletion primitives
+# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and
+# FLUSHDB commands, in order to reclaim memory in background. Those commands
+# are executed in constant time. Another thread will incrementally free the
+# object in the background as fast as possible.
+#
+# DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.
+# It's up to the design of the application to understand when it is a good
+# idea to use one or the other. However the Redis server sometimes has to
+# delete keys or flush the whole database as a side effect of other operations.
+# Specifically Redis deletes objects independently of a user call in the
+# following scenarios:
+#
+# 1) On eviction, because of the maxmemory and maxmemory policy configurations,
+#    in order to make room for new data, without going over the specified
+#    memory limit.
+# 2) Because of expire: when a key with an associated time to live (see the
+#    EXPIRE command) must be deleted from memory.
+# 3) Because of a side effect of a command that stores data on a key that may
+#    already exist. For example the RENAME command may delete the old key
+#    content when it is replaced with another one. Similarly SUNIONSTORE
+#    or SORT with STORE option may delete existing keys. The SET command
+#    itself removes any old content of the specified key in order to replace
+#    it with the specified string.
+# 4) During replication, when a replica performs a full resynchronization with
+#    its master, the content of the whole database is removed in order to
+#    load the RDB file just transferred.
+#
+# In all the above cases the default is to delete objects in a blocking way,
+# like if DEL was called. However you can configure each case specifically
+# in order to instead release memory in a non-blocking way like if UNLINK
+# was called, using the following configuration directives.
+
+lazyfree-lazy-eviction no
+lazyfree-lazy-expire no
+lazyfree-lazy-server-del no
+replica-lazy-flush no
+
+# It is also possible, for the case when to replace the user code DEL calls
+# with UNLINK calls is not easy, to modify the default behavior of the DEL
+# command to act exactly like UNLINK, using the following configuration
+# directive:
+
+lazyfree-lazy-user-del no
+
+# FLUSHDB, FLUSHALL, and SCRIPT FLUSH support both asynchronous and synchronous
+# deletion, which can be controlled by passing the [SYNC|ASYNC] flags into the
+# commands. When neither flag is passed, this directive will be used to determine
+# if the data should be deleted asynchronously.
+
+lazyfree-lazy-user-flush no
+
+################################ THREADED I/O #################################
+
+# Redis is mostly single threaded, however there are certain threaded
+# operations such as UNLINK, slow I/O accesses and other things that are
+# performed on side threads.
+#
+# Now it is also possible to handle Redis clients socket reads and writes
+# in different I/O threads. Since especially writing is so slow, normally
+# Redis users use pipelining in order to speed up the Redis performances per
+# core, and spawn multiple instances in order to scale more. Using I/O
+# threads it is possible to easily speedup two times Redis without resorting
+# to pipelining nor sharding of the instance.
+#
+# By default threading is disabled, we suggest enabling it only in machines
+# that have at least 4 or more cores, leaving at least one spare core.
+# Using more than 8 threads is unlikely to help much. We also recommend using
+# threaded I/O only if you actually have performance problems, with Redis
+# instances being able to use a quite big percentage of CPU time, otherwise
+# there is no point in using this feature.
+#
+# So for instance if you have a four cores boxes, try to use 2 or 3 I/O
+# threads, if you have a 8 cores, try to use 6 threads. In order to
+# enable I/O threads use the following configuration directive:
+#
+# io-threads 4
+#
+# Setting io-threads to 1 will just use the main thread as usual.
+# When I/O threads are enabled, we only use threads for writes, that is
+# to thread the write(2) syscall and transfer the client buffers to the
+# socket. However it is also possible to enable threading of reads and
+# protocol parsing using the following configuration directive, by setting
+# it to yes:
+#
+# io-threads-do-reads no
+#
+# Usually threading reads doesn't help much.
+#
+# NOTE 1: This configuration directive cannot be changed at runtime via
+# CONFIG SET. Aso this feature currently does not work when SSL is
+# enabled.
+#
+# NOTE 2: If you want to test the Redis speedup using redis-benchmark, make
+# sure you also run the benchmark itself in threaded mode, using the
+# --threads option to match the number of Redis threads, otherwise you'll not
+# be able to notice the improvements.
+
+############################ KERNEL OOM CONTROL ##############################
+
+# On Linux, it is possible to hint the kernel OOM killer on what processes
+# should be killed first when out of memory.
+#
+# Enabling this feature makes Redis actively control the oom_score_adj value
+# for all its processes, depending on their role. The default scores will
+# attempt to have background child processes killed before all others, and
+# replicas killed before masters.
+#
+# Redis supports three options:
+#
+# no:       Don't make changes to oom-score-adj (default).
+# yes:      Alias to "relative" see below.
+# absolute: Values in oom-score-adj-values are written as is to the kernel.
+# relative: Values are used relative to the initial value of oom_score_adj when
+#           the server starts and are then clamped to a range of -1000 to 1000.
+#           Because typically the initial value is 0, they will often match the
+#           absolute values.
+oom-score-adj no
+
+# When oom-score-adj is used, this directive controls the specific values used
+# for master, replica and background child processes. Values range -2000 to
+# 2000 (higher means more likely to be killed).
+#
+# Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)
+# can freely increase their value, but not decrease it below its initial
+# settings. This means that setting oom-score-adj to "relative" and setting the
+# oom-score-adj-values to positive values will always succeed.
+oom-score-adj-values 0 200 800
+
+
+#################### KERNEL transparent hugepage CONTROL ######################
+
+# Usually the kernel Transparent Huge Pages control is set to "madvise" or
+# or "never" by default (/sys/kernel/mm/transparent_hugepage/enabled), in which
+# case this config has no effect. On systems in which it is set to "always",
+# redis will attempt to disable it specifically for the redis process in order
+# to avoid latency problems specifically with fork(2) and CoW.
+# If for some reason you prefer to keep it enabled, you can set this config to
+# "no" and the kernel global to "always".
+
+disable-thp yes
+
+############################## APPEND ONLY MODE ###############################
+
+# By default Redis asynchronously dumps the dataset on disk. This mode is
+# good enough in many applications, but an issue with the Redis process or
+# a power outage may result into a few minutes of writes lost (depending on
+# the configured save points).
+#
+# The Append Only File is an alternative persistence mode that provides
+# much better durability. For instance using the default data fsync policy
+# (see later in the config file) Redis can lose just one second of writes in a
+# dramatic event like a server power outage, or a single write if something
+# wrong with the Redis process itself happens, but the operating system is
+# still running correctly.
+#
+# AOF and RDB persistence can be enabled at the same time without problems.
+# If the AOF is enabled on startup Redis will load the AOF, that is the file
+# with the better durability guarantees.
+#
+# Please check https://redis.io/topics/persistence for more information.
+
+appendonly no
+
+# The name of the append only file (default: "appendonly.aof")
+
+appendfilename "appendonly.aof"
+
+# The fsync() call tells the Operating System to actually write data on disk
+# instead of waiting for more data in the output buffer. Some OS will really flush
+# data on disk, some other OS will just try to do it ASAP.
+#
+# Redis supports three different modes:
+#
+# no: don't fsync, just let the OS flush the data when it wants. Faster.
+# always: fsync after every write to the append only log. Slow, Safest.
+# everysec: fsync only one time every second. Compromise.
+#
+# The default is "everysec", as that's usually the right compromise between
+# speed and data safety. It's up to you to understand if you can relax this to
+# "no" that will let the operating system flush the output buffer when
+# it wants, for better performances (but if you can live with the idea of
+# some data loss consider the default persistence mode that's snapshotting),
+# or on the contrary, use "always" that's very slow but a bit safer than
+# everysec.
+#
+# More details please check the following article:
+# http://antirez.com/post/redis-persistence-demystified.html
+#
+# If unsure, use "everysec".
+
+# appendfsync always
+appendfsync everysec
+# appendfsync no
+
+# When the AOF fsync policy is set to always or everysec, and a background
+# saving process (a background save or AOF log background rewriting) is
+# performing a lot of I/O against the disk, in some Linux configurations
+# Redis may block too long on the fsync() call. Note that there is no fix for
+# this currently, as even performing fsync in a different thread will block
+# our synchronous write(2) call.
+#
+# In order to mitigate this problem it's possible to use the following option
+# that will prevent fsync() from being called in the main process while a
+# BGSAVE or BGREWRITEAOF is in progress.
+#
+# This means that while another child is saving, the durability of Redis is
+# the same as "appendfsync none". In practical terms, this means that it is
+# possible to lose up to 30 seconds of log in the worst scenario (with the
+# default Linux settings).
+#
+# If you have latency problems turn this to "yes". Otherwise leave it as
+# "no" that is the safest pick from the point of view of durability.
+
+no-appendfsync-on-rewrite no
+
+# Automatic rewrite of the append only file.
+# Redis is able to automatically rewrite the log file implicitly calling
+# BGREWRITEAOF when the AOF log size grows by the specified percentage.
+#
+# This is how it works: Redis remembers the size of the AOF file after the
+# latest rewrite (if no rewrite has happened since the restart, the size of
+# the AOF at startup is used).
+#
+# This base size is compared to the current size. If the current size is
+# bigger than the specified percentage, the rewrite is triggered. Also
+# you need to specify a minimal size for the AOF file to be rewritten, this
+# is useful to avoid rewriting the AOF file even if the percentage increase
+# is reached but it is still pretty small.
+#
+# Specify a percentage of zero in order to disable the automatic AOF
+# rewrite feature.
+
+auto-aof-rewrite-percentage 100
+auto-aof-rewrite-min-size 64mb
+
+# An AOF file may be found to be truncated at the end during the Redis
+# startup process, when the AOF data gets loaded back into memory.
+# This may happen when the system where Redis is running
+# crashes, especially when an ext4 filesystem is mounted without the
+# data=ordered option (however this can't happen when Redis itself
+# crashes or aborts but the operating system still works correctly).
+#
+# Redis can either exit with an error when this happens, or load as much
+# data as possible (the default now) and start if the AOF file is found
+# to be truncated at the end. The following option controls this behavior.
+#
+# If aof-load-truncated is set to yes, a truncated AOF file is loaded and
+# the Redis server starts emitting a log to inform the user of the event.
+# Otherwise if the option is set to no, the server aborts with an error
+# and refuses to start. When the option is set to no, the user requires
+# to fix the AOF file using the "redis-check-aof" utility before to restart
+# the server.
+#
+# Note that if the AOF file will be found to be corrupted in the middle
+# the server will still exit with an error. This option only applies when
+# Redis will try to read more data from the AOF file but not enough bytes
+# will be found.
+aof-load-truncated yes
+
+# When rewriting the AOF file, Redis is able to use an RDB preamble in the
+# AOF file for faster rewrites and recoveries. When this option is turned
+# on the rewritten AOF file is composed of two different stanzas:
+#
+#   [RDB file][AOF tail]
+#
+# When loading, Redis recognizes that the AOF file starts with the "REDIS"
+# string and loads the prefixed RDB file, then continues loading the AOF
+# tail.
+aof-use-rdb-preamble yes
+
+################################ LUA SCRIPTING  ###############################
+
+# Max execution time of a Lua script in milliseconds.
+#
+# If the maximum execution time is reached Redis will log that a script is
+# still in execution after the maximum allowed time and will start to
+# reply to queries with an error.
+#
+# When a long running script exceeds the maximum execution time only the
+# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be
+# used to stop a script that did not yet call any write commands. The second
+# is the only way to shut down the server in the case a write command was
+# already issued by the script but the user doesn't want to wait for the natural
+# termination of the script.
+#
+# Set it to 0 or a negative value for unlimited execution without warnings.
+lua-time-limit 5000
+
+################################ REDIS CLUSTER  ###############################
+
+# Normal Redis instances can't be part of a Redis Cluster; only nodes that are
+# started as cluster nodes can. In order to start a Redis instance as a
+# cluster node enable the cluster support uncommenting the following:
+#
+# cluster-enabled yes
+
+# Every cluster node has a cluster configuration file. This file is not
+# intended to be edited by hand. It is created and updated by Redis nodes.
+# Every Redis Cluster node requires a different cluster configuration file.
+# Make sure that instances running in the same system do not have
+# overlapping cluster configuration file names.
+#
+# cluster-config-file nodes-6379.conf
+
+# Cluster node timeout is the amount of milliseconds a node must be unreachable
+# for it to be considered in failure state.
+# Most other internal time limits are a multiple of the node timeout.
+#
+# cluster-node-timeout 15000
+
+# A replica of a failing master will avoid to start a failover if its data
+# looks too old.
+#
+# There is no simple way for a replica to actually have an exact measure of
+# its "data age", so the following two checks are performed:
+#
+# 1) If there are multiple replicas able to failover, they exchange messages
+#    in order to try to give an advantage to the replica with the best
+#    replication offset (more data from the master processed).
+#    Replicas will try to get their rank by offset, and apply to the start
+#    of the failover a delay proportional to their rank.
+#
+# 2) Every single replica computes the time of the last interaction with
+#    its master. This can be the last ping or command received (if the master
+#    is still in the "connected" state), or the time that elapsed since the
+#    disconnection with the master (if the replication link is currently down).
+#    If the last interaction is too old, the replica will not try to failover
+#    at all.
+#
+# The point "2" can be tuned by user. Specifically a replica will not perform
+# the failover if, since the last interaction with the master, the time
+# elapsed is greater than:
+#
+#   (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period
+#
+# So for example if node-timeout is 30 seconds, and the cluster-replica-validity-factor
+# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the
+# replica will not try to failover if it was not able to talk with the master
+# for longer than 310 seconds.
+#
+# A large cluster-replica-validity-factor may allow replicas with too old data to failover
+# a master, while a too small value may prevent the cluster from being able to
+# elect a replica at all.
+#
+# For maximum availability, it is possible to set the cluster-replica-validity-factor
+# to a value of 0, which means, that replicas will always try to failover the
+# master regardless of the last time they interacted with the master.
+# (However they'll always try to apply a delay proportional to their
+# offset rank).
+#
+# Zero is the only value able to guarantee that when all the partitions heal
+# the cluster will always be able to continue.
+#
+# cluster-replica-validity-factor 10
+
+# Cluster replicas are able to migrate to orphaned masters, that are masters
+# that are left without working replicas. This improves the cluster ability
+# to resist to failures as otherwise an orphaned master can't be failed over
+# in case of failure if it has no working replicas.
+#
+# Replicas migrate to orphaned masters only if there are still at least a
+# given number of other working replicas for their old master. This number
+# is the "migration barrier". A migration barrier of 1 means that a replica
+# will migrate only if there is at least 1 other working replica for its master
+# and so forth. It usually reflects the number of replicas you want for every
+# master in your cluster.
+#
+# Default is 1 (replicas migrate only if their masters remain with at least
+# one replica). To disable migration just set it to a very large value or
+# set cluster-allow-replica-migration to 'no'.
+# A value of 0 can be set but is useful only for debugging and dangerous
+# in production.
+#
+# cluster-migration-barrier 1
+
+# Turning off this option allows to use less automatic cluster configuration.
+# It both disables migration to orphaned masters and migration from masters
+# that became empty.
+#
+# Default is 'yes' (allow automatic migrations).
+#
+# cluster-allow-replica-migration yes
+
+# By default Redis Cluster nodes stop accepting queries if they detect there
+# is at least a hash slot uncovered (no available node is serving it).
+# This way if the cluster is partially down (for example a range of hash slots
+# are no longer covered) all the cluster becomes, eventually, unavailable.
+# It automatically returns available as soon as all the slots are covered again.
+#
+# However sometimes you want the subset of the cluster which is working,
+# to continue to accept queries for the part of the key space that is still
+# covered. In order to do so, just set the cluster-require-full-coverage
+# option to no.
+#
+# cluster-require-full-coverage yes
+
+# This option, when set to yes, prevents replicas from trying to failover its
+# master during master failures. However the replica can still perform a
+# manual failover, if forced to do so.
+#
+# This is useful in different scenarios, especially in the case of multiple
+# data center operations, where we want one side to never be promoted if not
+# in the case of a total DC failure.
+#
+# cluster-replica-no-failover no
+
+# This option, when set to yes, allows nodes to serve read traffic while the
+# the cluster is in a down state, as long as it believes it owns the slots.
+#
+# This is useful for two cases.  The first case is for when an application
+# doesn't require consistency of data during node failures or network partitions.
+# One example of this is a cache, where as long as the node has the data it
+# should be able to serve it.
+#
+# The second use case is for configurations that don't meet the recommended
+# three shards but want to enable cluster mode and scale later. A
+# master outage in a 1 or 2 shard configuration causes a read/write outage to the
+# entire cluster without this option set, with it set there is only a write outage.
+# Without a quorum of masters, slot ownership will not change automatically.
+#
+# cluster-allow-reads-when-down no
+
+# In order to setup your cluster make sure to read the documentation
+# available at https://redis.io web site.
+
+########################## CLUSTER DOCKER/NAT support  ########################
+
+# In certain deployments, Redis Cluster nodes address discovery fails, because
+# addresses are NAT-ted or because ports are forwarded (the typical case is
+# Docker and other containers).
+#
+# In order to make Redis Cluster working in such environments, a static
+# configuration where each node knows its public address is needed. The
+# following four options are used for this scope, and are:
+#
+# * cluster-announce-ip
+# * cluster-announce-port
+# * cluster-announce-tls-port
+# * cluster-announce-bus-port
+#
+# Each instructs the node about its address, client ports (for connections
+# without and with TLS) and cluster message bus port. The information is then
+# published in the header of the bus packets so that other nodes will be able to
+# correctly map the address of the node publishing the information.
+#
+# If cluster-tls is set to yes and cluster-announce-tls-port is omitted or set
+# to zero, then cluster-announce-port refers to the TLS port. Note also that
+# cluster-announce-tls-port has no effect if cluster-tls is set to no.
+#
+# If the above options are not used, the normal Redis Cluster auto-detection
+# will be used instead.
+#
+# Note that when remapped, the bus port may not be at the fixed offset of
+# clients port + 10000, so you can specify any port and bus-port depending
+# on how they get remapped. If the bus-port is not set, a fixed offset of
+# 10000 will be used as usual.
+#
+# Example:
+#
+# cluster-announce-ip 10.1.1.5
+# cluster-announce-tls-port 6379
+# cluster-announce-port 0
+# cluster-announce-bus-port 6380
+
+################################## SLOW LOG ###################################
+
+# The Redis Slow Log is a system to log queries that exceeded a specified
+# execution time. The execution time does not include the I/O operations
+# like talking with the client, sending the reply and so forth,
+# but just the time needed to actually execute the command (this is the only
+# stage of command execution where the thread is blocked and can not serve
+# other requests in the meantime).
+#
+# You can configure the slow log with two parameters: one tells Redis
+# what is the execution time, in microseconds, to exceed in order for the
+# command to get logged, and the other parameter is the length of the
+# slow log. When a new command is logged the oldest one is removed from the
+# queue of logged commands.
+
+# The following time is expressed in microseconds, so 1000000 is equivalent
+# to one second. Note that a negative number disables the slow log, while
+# a value of zero forces the logging of every command.
+slowlog-log-slower-than 10000
+
+# There is no limit to this length. Just be aware that it will consume memory.
+# You can reclaim memory used by the slow log with SLOWLOG RESET.
+slowlog-max-len 128
+
+################################ LATENCY MONITOR ##############################
+
+# The Redis latency monitoring subsystem samples different operations
+# at runtime in order to collect data related to possible sources of
+# latency of a Redis instance.
+#
+# Via the LATENCY command this information is available to the user that can
+# print graphs and obtain reports.
+#
+# The system only logs operations that were performed in a time equal or
+# greater than the amount of milliseconds specified via the
+# latency-monitor-threshold configuration directive. When its value is set
+# to zero, the latency monitor is turned off.
+#
+# By default latency monitoring is disabled since it is mostly not needed
+# if you don't have latency issues, and collecting data has a performance
+# impact, that while very small, can be measured under big load. Latency
+# monitoring can easily be enabled at runtime using the command
+# "CONFIG SET latency-monitor-threshold <milliseconds>" if needed.
+latency-monitor-threshold 0
+
+############################# EVENT NOTIFICATION ##############################
+
+# Redis can notify Pub/Sub clients about events happening in the key space.
+# This feature is documented at https://redis.io/topics/notifications
+#
+# For instance if keyspace events notification is enabled, and a client
+# performs a DEL operation on key "foo" stored in the Database 0, two
+# messages will be published via Pub/Sub:
+#
+# PUBLISH __keyspace@0__:foo del
+# PUBLISH __keyevent@0__:del foo
+#
+# It is possible to select the events that Redis will notify among a set
+# of classes. Every class is identified by a single character:
+#
+#  K     Keyspace events, published with __keyspace@<db>__ prefix.
+#  E     Keyevent events, published with __keyevent@<db>__ prefix.
+#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...
+#  $     String commands
+#  l     List commands
+#  s     Set commands
+#  h     Hash commands
+#  z     Sorted set commands
+#  x     Expired events (events generated every time a key expires)
+#  e     Evicted events (events generated when a key is evicted for maxmemory)
+#  t     Stream commands
+#  d     Module key type events
+#  m     Key-miss events (Note: It is not included in the 'A' class)
+#  A     Alias for g$lshzxetd, so that the "AKE" string means all the events
+#        (Except key-miss events which are excluded from 'A' due to their
+#         unique nature).
+#
+#  The "notify-keyspace-events" takes as argument a string that is composed
+#  of zero or multiple characters. The empty string means that notifications
+#  are disabled.
+#
+#  Example: to enable list and generic events, from the point of view of the
+#           event name, use:
+#
+#  notify-keyspace-events Elg
+#
+#  Example 2: to get the stream of the expired keys subscribing to channel
+#             name __keyevent@0__:expired use:
+#
+#  notify-keyspace-events Ex
+#
+#  By default all notifications are disabled because most users don't need
+#  this feature and the feature has some overhead. Note that if you don't
+#  specify at least one of K or E, no events will be delivered.
+notify-keyspace-events ""
+
+############################### GOPHER SERVER #################################
+
+# Redis contains an implementation of the Gopher protocol, as specified in
+# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).
+#
+# The Gopher protocol was very popular in the late '90s. It is an alternative
+# to the web, and the implementation both server and client side is so simple
+# that the Redis server has just 100 lines of code in order to implement this
+# support.
+#
+# What do you do with Gopher nowadays? Well Gopher never *really* died, and
+# lately there is a movement in order for the Gopher more hierarchical content
+# composed of just plain text documents to be resurrected. Some want a simpler
+# internet, others believe that the mainstream internet became too much
+# controlled, and it's cool to create an alternative space for people that
+# want a bit of fresh air.
+#
+# Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol
+# as a gift.
+#
+# --- HOW IT WORKS? ---
+#
+# The Redis Gopher support uses the inline protocol of Redis, and specifically
+# two kind of inline requests that were anyway illegal: an empty request
+# or any request that starts with "/" (there are no Redis commands starting
+# with such a slash). Normal RESP2/RESP3 requests are completely out of the
+# path of the Gopher protocol implementation and are served as usual as well.
+#
+# If you open a connection to Redis when Gopher is enabled and send it
+# a string like "/foo", if there is a key named "/foo" it is served via the
+# Gopher protocol.
+#
+# In order to create a real Gopher "hole" (the name of a Gopher site in Gopher
+# talking), you likely need a script like the following:
+#
+#   https://github.com/antirez/gopher2redis
+#
+# --- SECURITY WARNING ---
+#
+# If you plan to put Redis on the internet in a publicly accessible address
+# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.
+# Once a password is set:
+#
+#   1. The Gopher server (when enabled, not by default) will still serve
+#      content via Gopher.
+#   2. However other commands cannot be called before the client will
+#      authenticate.
+#
+# So use the 'requirepass' option to protect your instance.
+#
+# Note that Gopher is not currently supported when 'io-threads-do-reads'
+# is enabled.
+#
+# To enable Gopher support, uncomment the following line and set the option
+# from no (the default) to yes.
+#
+# gopher-enabled no
+
+############################### ADVANCED CONFIG ###############################
+
+# Hashes are encoded using a memory efficient data structure when they have a
+# small number of entries, and the biggest entry does not exceed a given
+# threshold. These thresholds can be configured using the following directives.
+hash-max-ziplist-entries 512
+hash-max-ziplist-value 64
+
+# Lists are also encoded in a special way to save a lot of space.
+# The number of entries allowed per internal list node can be specified
+# as a fixed maximum size or a maximum number of elements.
+# For a fixed maximum size, use -5 through -1, meaning:
+# -5: max size: 64 Kb  <-- not recommended for normal workloads
+# -4: max size: 32 Kb  <-- not recommended
+# -3: max size: 16 Kb  <-- probably not recommended
+# -2: max size: 8 Kb   <-- good
+# -1: max size: 4 Kb   <-- good
+# Positive numbers mean store up to _exactly_ that number of elements
+# per list node.
+# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),
+# but if your use case is unique, adjust the settings as necessary.
+list-max-ziplist-size -2
+
+# Lists may also be compressed.
+# Compress depth is the number of quicklist ziplist nodes from *each* side of
+# the list to *exclude* from compression.  The head and tail of the list
+# are always uncompressed for fast push/pop operations.  Settings are:
+# 0: disable all list compression
+# 1: depth 1 means "don't start compressing until after 1 node into the list,
+#    going from either the head or tail"
+#    So: [head]->node->node->...->node->[tail]
+#    [head], [tail] will always be uncompressed; inner nodes will compress.
+# 2: [head]->[next]->node->node->...->node->[prev]->[tail]
+#    2 here means: don't compress head or head->next or tail->prev or tail,
+#    but compress all nodes between them.
+# 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail]
+# etc.
+list-compress-depth 0
+
+# Sets have a special encoding in just one case: when a set is composed
+# of just strings that happen to be integers in radix 10 in the range
+# of 64 bit signed integers.
+# The following configuration setting sets the limit in the size of the
+# set in order to use this special memory saving encoding.
+set-max-intset-entries 512
+
+# Similarly to hashes and lists, sorted sets are also specially encoded in
+# order to save a lot of space. This encoding is only used when the length and
+# elements of a sorted set are below the following limits:
+zset-max-ziplist-entries 128
+zset-max-ziplist-value 64
+
+# HyperLogLog sparse representation bytes limit. The limit includes the
+# 16 bytes header. When an HyperLogLog using the sparse representation crosses
+# this limit, it is converted into the dense representation.
+#
+# A value greater than 16000 is totally useless, since at that point the
+# dense representation is more memory efficient.
+#
+# The suggested value is ~ 3000 in order to have the benefits of
+# the space efficient encoding without slowing down too much PFADD,
+# which is O(N) with the sparse encoding. The value can be raised to
+# ~ 10000 when CPU is not a concern, but space is, and the data set is
+# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.
+hll-sparse-max-bytes 3000
+
+# Streams macro node max size / items. The stream data structure is a radix
+# tree of big nodes that encode multiple items inside. Using this configuration
+# it is possible to configure how big a single node can be in bytes, and the
+# maximum number of items it may contain before switching to a new node when
+# appending new stream entries. If any of the following settings are set to
+# zero, the limit is ignored, so for instance it is possible to set just a
+# max entries limit by setting max-bytes to 0 and max-entries to the desired
+# value.
+stream-node-max-bytes 4096
+stream-node-max-entries 100
+
+# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in
+# order to help rehashing the main Redis hash table (the one mapping top-level
+# keys to values). The hash table implementation Redis uses (see dict.c)
+# performs a lazy rehashing: the more operation you run into a hash table
+# that is rehashing, the more rehashing "steps" are performed, so if the
+# server is idle the rehashing is never complete and some more memory is used
+# by the hash table.
+#
+# The default is to use this millisecond 10 times every second in order to
+# actively rehash the main dictionaries, freeing memory when possible.
+#
+# If unsure:
+# use "activerehashing no" if you have hard latency requirements and it is
+# not a good thing in your environment that Redis can reply from time to time
+# to queries with 2 milliseconds delay.
+#
+# use "activerehashing yes" if you don't have such hard requirements but
+# want to free memory asap when possible.
+activerehashing yes
+
+# The client output buffer limits can be used to force disconnection of clients
+# that are not reading data from the server fast enough for some reason (a
+# common reason is that a Pub/Sub client can't consume messages as fast as the
+# publisher can produce them).
+#
+# The limit can be set differently for the three different classes of clients:
+#
+# normal -> normal clients including MONITOR clients
+# replica  -> replica clients
+# pubsub -> clients subscribed to at least one pubsub channel or pattern
+#
+# The syntax of every client-output-buffer-limit directive is the following:
+#
+# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
+#
+# A client is immediately disconnected once the hard limit is reached, or if
+# the soft limit is reached and remains reached for the specified number of
+# seconds (continuously).
+# So for instance if the hard limit is 32 megabytes and the soft limit is
+# 16 megabytes / 10 seconds, the client will get disconnected immediately
+# if the size of the output buffers reach 32 megabytes, but will also get
+# disconnected if the client reaches 16 megabytes and continuously overcomes
+# the limit for 10 seconds.
+#
+# By default normal clients are not limited because they don't receive data
+# without asking (in a push way), but just after a request, so only
+# asynchronous clients may create a scenario where data is requested faster
+# than it can read.
+#
+# Instead there is a default limit for pubsub and replica clients, since
+# subscribers and replicas receive data in a push fashion.
+#
+# Both the hard or the soft limit can be disabled by setting them to zero.
+client-output-buffer-limit normal 0 0 0
+client-output-buffer-limit replica 256mb 64mb 60
+client-output-buffer-limit pubsub 32mb 8mb 60
+
+# Client query buffers accumulate new commands. They are limited to a fixed
+# amount by default in order to avoid that a protocol desynchronization (for
+# instance due to a bug in the client) will lead to unbound memory usage in
+# the query buffer. However you can configure it here if you have very special
+# needs, such us huge multi/exec requests or alike.
+#
+# client-query-buffer-limit 1gb
+
+# In the Redis protocol, bulk requests, that are, elements representing single
+# strings, are normally limited to 512 mb. However you can change this limit
+# here, but must be 1mb or greater
+#
+# proto-max-bulk-len 512mb
+
+# Redis calls an internal function to perform many background tasks, like
+# closing connections of clients in timeout, purging expired keys that are
+# never requested, and so forth.
+#
+# Not all tasks are performed with the same frequency, but Redis checks for
+# tasks to perform according to the specified "hz" value.
+#
+# By default "hz" is set to 10. Raising the value will use more CPU when
+# Redis is idle, but at the same time will make Redis more responsive when
+# there are many keys expiring at the same time, and timeouts may be
+# handled with more precision.
+#
+# The range is between 1 and 500, however a value over 100 is usually not
+# a good idea. Most users should use the default of 10 and raise this up to
+# 100 only in environments where very low latency is required.
+hz 10
+
+# Normally it is useful to have an HZ value which is proportional to the
+# number of clients connected. This is useful in order, for instance, to
+# avoid too many clients are processed for each background task invocation
+# in order to avoid latency spikes.
+#
+# Since the default HZ value by default is conservatively set to 10, Redis
+# offers, and enables by default, the ability to use an adaptive HZ value
+# which will temporarily raise when there are many connected clients.
+#
+# When dynamic HZ is enabled, the actual configured HZ will be used
+# as a baseline, but multiples of the configured HZ value will be actually
+# used as needed once more clients are connected. In this way an idle
+# instance will use very little CPU time while a busy instance will be
+# more responsive.
+dynamic-hz yes
+
+# When a child rewrites the AOF file, if the following option is enabled
+# the file will be fsync-ed every 32 MB of data generated. This is useful
+# in order to commit the file to the disk more incrementally and avoid
+# big latency spikes.
+aof-rewrite-incremental-fsync yes
+
+# When redis saves RDB file, if the following option is enabled
+# the file will be fsync-ed every 32 MB of data generated. This is useful
+# in order to commit the file to the disk more incrementally and avoid
+# big latency spikes.
+rdb-save-incremental-fsync yes
+
+# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good
+# idea to start with the default settings and only change them after investigating
+# how to improve the performances and how the keys LFU change over time, which
+# is possible to inspect via the OBJECT FREQ command.
+#
+# There are two tunable parameters in the Redis LFU implementation: the
+# counter logarithm factor and the counter decay time. It is important to
+# understand what the two parameters mean before changing them.
+#
+# The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis
+# uses a probabilistic increment with logarithmic behavior. Given the value
+# of the old counter, when a key is accessed, the counter is incremented in
+# this way:
+#
+# 1. A random number R between 0 and 1 is extracted.
+# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).
+# 3. The counter is incremented only if R < P.
+#
+# The default lfu-log-factor is 10. This is a table of how the frequency
+# counter changes with a different number of accesses with different
+# logarithmic factors:
+#
+# +--------+------------+------------+------------+------------+------------+
+# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |
+# +--------+------------+------------+------------+------------+------------+
+# | 0      | 104        | 255        | 255        | 255        | 255        |
+# +--------+------------+------------+------------+------------+------------+
+# | 1      | 18         | 49         | 255        | 255        | 255        |
+# +--------+------------+------------+------------+------------+------------+
+# | 10     | 10         | 18         | 142        | 255        | 255        |
+# +--------+------------+------------+------------+------------+------------+
+# | 100    | 8          | 11         | 49         | 143        | 255        |
+# +--------+------------+------------+------------+------------+------------+
+#
+# NOTE: The above table was obtained by running the following commands:
+#
+#   redis-benchmark -n 1000000 incr foo
+#   redis-cli object freq foo
+#
+# NOTE 2: The counter initial value is 5 in order to give new objects a chance
+# to accumulate hits.
+#
+# The counter decay time is the time, in minutes, that must elapse in order
+# for the key counter to be divided by two (or decremented if it has a value
+# less <= 10).
+#
+# The default value for the lfu-decay-time is 1. A special value of 0 means to
+# decay the counter every time it happens to be scanned.
+#
+# lfu-log-factor 10
+# lfu-decay-time 1
+
+########################### ACTIVE DEFRAGMENTATION #######################
+#
+# What is active defragmentation?
+# -------------------------------
+#
+# Active (online) defragmentation allows a Redis server to compact the
+# spaces left between small allocations and deallocations of data in memory,
+# thus allowing to reclaim back memory.
+#
+# Fragmentation is a natural process that happens with every allocator (but
+# less so with Jemalloc, fortunately) and certain workloads. Normally a server
+# restart is needed in order to lower the fragmentation, or at least to flush
+# away all the data and create it again. However thanks to this feature
+# implemented by Oran Agra for Redis 4.0 this process can happen at runtime
+# in a "hot" way, while the server is running.
+#
+# Basically when the fragmentation is over a certain level (see the
+# configuration options below) Redis will start to create new copies of the
+# values in contiguous memory regions by exploiting certain specific Jemalloc
+# features (in order to understand if an allocation is causing fragmentation
+# and to allocate it in a better place), and at the same time, will release the
+# old copies of the data. This process, repeated incrementally for all the keys
+# will cause the fragmentation to drop back to normal values.
+#
+# Important things to understand:
+#
+# 1. This feature is disabled by default, and only works if you compiled Redis
+#    to use the copy of Jemalloc we ship with the source code of Redis.
+#    This is the default with Linux builds.
+#
+# 2. You never need to enable this feature if you don't have fragmentation
+#    issues.
+#
+# 3. Once you experience fragmentation, you can enable this feature when
+#    needed with the command "CONFIG SET activedefrag yes".
+#
+# The configuration parameters are able to fine tune the behavior of the
+# defragmentation process. If you are not sure about what they mean it is
+# a good idea to leave the defaults untouched.
+
+# Enabled active defragmentation
+# activedefrag no
+
+# Minimum amount of fragmentation waste to start active defrag
+# active-defrag-ignore-bytes 100mb
+
+# Minimum percentage of fragmentation to start active defrag
+# active-defrag-threshold-lower 10
+
+# Maximum percentage of fragmentation at which we use maximum effort
+# active-defrag-threshold-upper 100
+
+# Minimal effort for defrag in CPU percentage, to be used when the lower
+# threshold is reached
+# active-defrag-cycle-min 1
+
+# Maximal effort for defrag in CPU percentage, to be used when the upper
+# threshold is reached
+# active-defrag-cycle-max 25
+
+# Maximum number of set/hash/zset/list fields that will be processed from
+# the main dictionary scan
+# active-defrag-max-scan-fields 1000
+
+# Jemalloc background thread for purging will be enabled by default
+jemalloc-bg-thread yes
+
+# It is possible to pin different threads and processes of Redis to specific
+# CPUs in your system, in order to maximize the performances of the server.
+# This is useful both in order to pin different Redis threads in different
+# CPUs, but also in order to make sure that multiple Redis instances running
+# in the same host will be pinned to different CPUs.
+#
+# Normally you can do this using the "taskset" command, however it is also
+# possible to this via Redis configuration directly, both in Linux and FreeBSD.
+#
+# You can pin the server/IO threads, bio threads, aof rewrite child process, and
+# the bgsave child process. The syntax to specify the cpu list is the same as
+# the taskset command:
+#
+# Set redis server/io threads to cpu affinity 0,2,4,6:
+# server_cpulist 0-7:2
+#
+# Set bio threads to cpu affinity 1,3:
+# bio_cpulist 1,3
+#
+# Set aof rewrite child process to cpu affinity 8,9,10,11:
+# aof_rewrite_cpulist 8-11
+#
+# Set bgsave child process to cpu affinity 1,10,11
+# bgsave_cpulist 1,10-11
+
+# In some cases redis will emit warnings and even refuse to start if it detects
+# that the system is in bad state, it is possible to suppress these warnings
+# by setting the following config which takes a space delimited list of warnings
+# to suppress
+#
+# ignore-warnings ARM64-COW-BUG
Index: redis/src/main/java/com/ly/redis/service/impl/UserServiceImpl.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- redis/src/main/java/com/ly/redis/service/impl/UserServiceImpl.java	(date 1621184874744)
+++ redis/src/main/java/com/ly/redis/service/impl/UserServiceImpl.java	(date 1621184874744)
@@ -0,0 +1,183 @@
+package com.ly.redis.service.impl;
+
+import com.ly.redis.mapper.UserMapper;
+import com.ly.redis.dto.UserDTO;
+import com.ly.redis.entity.UserDO;
+import com.ly.redis.enums.DeleteFlagEnum;
+import com.ly.redis.service.UserService;
+import com.ly.redis.utils.BeanUtils;
+import lombok.extern.slf4j.Slf4j;
+import org.springframework.data.redis.core.RedisTemplate;
+import org.springframework.stereotype.Service;
+
+import javax.annotation.Resource;
+
+/**
+ * @author luoyong
+ *  * @create 2021-05-16 10:55 下午
+ *  * @last modify by [luoyong 2021-05-16 10:55 下午]
+ * @Description:
+ **/
+@Service
+@Slf4j
+public class UserServiceImpl implements UserService {
+
+    public static final String CACHE_KEY_USER = "user:";
+
+    @Resource
+    private UserMapper userDAO;
+
+    @Resource
+    private RedisTemplate redisTemplate;
+
+    /**
+     * @param userDTO
+     * @return
+     * @Description: 新增用户信息
+     * @author luoyong
+     * @create 10:59 下午 2021/5/16
+     * @last modify by [LuoYong 10:59 下午 2021/5/16 ]
+     */
+    @Override
+    public Boolean addUser(UserDTO userDTO) {
+        if (null == userDTO) {
+            log.info("用户信息为null");
+            return false;
+        }
+        UserDO userDO = BeanUtils.clone(userDTO, UserDO.class);
+        //1:入库
+        int i = userDAO.insertSelective(userDO);
+
+        if (i > 0) {
+            //2:需要再查询一下mysql 确保数据已经落库了
+            userDO = userDAO.selectByPrimaryKey(userDO.getId());
+
+            //3: 存入redis 完成新增功能的数据一致性
+            String key = CACHE_KEY_USER + userDO.getId();
+            redisTemplate.opsForValue().set(key, userDO);
+        }
+        return true;
+    }
+
+    /**
+     * @param userId
+     * @return
+     * @Description: 删除用户信息
+     * @author luoyong
+     * @create 11:00 下午 2021/5/16
+     * @last modify by [LuoYong 11:00 下午 2021/5/16 ]
+     */
+    @Override
+    public Boolean delUser(Long userId) {
+        UserDO userDO = new UserDO();
+        userDO.setId(userId);
+        userDO.setIsDeleted(DeleteFlagEnum.DELETED.getValue());
+        int i = userDAO.updateByPrimaryKeySelective(userDO);
+        if (i > 0) {
+            String key = CACHE_KEY_USER + userId;
+            redisTemplate.delete(key);
+        }
+        return true;
+    }
+
+    /**
+     * @param userDTO
+     * @return java.lang.Boolean
+     * @Description: 更新用户信息
+     * @author luoyong
+     * @create 11:01 下午 2021/5/16
+     * @last modify by [LuoYong 11:01 下午 2021/5/16 ]
+     */
+    @Override
+    public Boolean updateUser(UserDTO userDTO) {
+        if (null == userDTO) {
+            log.info("用户信息为null");
+            return false;
+        }
+        UserDO userDO = BeanUtils.clone(userDTO, UserDO.class);
+        int i = userDAO.updateByPrimaryKeySelective(userDO);
+        if (i > 0) {
+            //2:需要再查询一下mysql 确保数据已经落库了
+            userDO = userDAO.selectByPrimaryKey(userDO.getId());
+
+            //3: 将捞出来的user存进redis，完成修改
+            String key = CACHE_KEY_USER + userDO.getId();
+            redisTemplate.opsForValue().set(key, userDO);
+        }
+        return null;
+    }
+
+    /**
+     * @param userId
+     * @return
+     * @Description: 根据用户id获取用户信息
+     * 业务逻辑并没有写错，对于小厂中厂(QPS《=1000)可以使用，但是大厂不行
+     * @author luoyong
+     * @create 11:01 下午 2021/5/16
+     * @last modify by [LuoYong 11:01 下午 2021/5/16 ]
+     */
+    @Override
+    public UserDTO getUserInfo(Long userId) {
+        UserDTO userDTO = null;
+        String key = CACHE_KEY_USER + userId;
+
+        //1 先从redis里面查询，如果有直接返回结果，如果没有再去查询mysql
+        UserDO userDO = (UserDO) redisTemplate.opsForValue().get(key);
+
+        if (null == userDO) {
+            //2 redis里面无，继续查询mysql
+            userDO = userDAO.selectByPrimaryKey(userId);
+            if (null == userDO) {
+                //3.1 redis+mysql 都无数据
+                return null;
+            } else {
+                //3.2 mysql有，需要将数据写回redis，保证下一次的缓存命中率
+                redisTemplate.opsForValue().set(key, userDO);
+            }
+        }
+        if (null != userDO) {
+            userDTO = BeanUtils.clone(userDO, UserDTO.class);
+        }
+        return userDTO;
+    }
+
+    /**
+     * @param userId
+     * @return
+     * @Description: 根据用户id获取用户信息 高可用QPS
+     * @author luoyong
+     * @create 11:01 下午 2021/5/16
+     * @last modify by [LuoYong 11:01 下午 2021/5/16 ]
+     */
+    @Override
+    public UserDTO getUserInfo2(Long userId) {
+        UserDTO userDTO = null;
+        String key = CACHE_KEY_USER + userId;
+
+        //1 先从redis里面查询，如果有直接返回结果，如果没有再去查询mysql
+        UserDO userDO = (UserDO) redisTemplate.opsForValue().get(key);
+
+        if (null == userDO) {
+            //2 大厂用，对于高QPS的优化，进来就先加锁，保证一个请求操作，让外面的redis等待一下，避免击穿mysql
+            synchronized (UserServiceImpl.class) {
+                //3 二次查redis还是null，可以去查mysql了(mysql默认有数据)
+                userDO = (UserDO) redisTemplate.opsForValue().get(key);
+                if (null == userDO) {
+                    //4 询mysql拿数据
+                    userDO = userDAO.selectByPrimaryKey(userId);
+                    if (null == userDO) {
+                        //4 redis+mysql 都无数据
+                        return null;
+                    } else {
+                        //5 mysql里面有数据的，需要回写redis，完成数据一致性的同步工作
+                        redisTemplate.opsForValue().set(key, userDO);
+                    }
+                }
+            }
+        }
+        if (null != userDO) {
+            userDTO = BeanUtils.clone(userDO, UserDTO.class);
+        }
+        return userDTO;
+    }
+}
